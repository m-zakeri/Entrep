% =================================================================
% فایل chapter4_21.tex (اصلاح نهایی و رفع خطای Label تکراری)
% =================================================================

\clearpage

% -----------------------------------------------------------------
% تنظیمات شماره‌گذاری
% -----------------------------------------------------------------
\setcounter{section}{21} 
\setcounter{subsection}{0} 
\renewcommand{\thesubsection}{21.\arabic{subsection}} 

% -----------------------------------------------------------------
% تیتر اصلی
% -----------------------------------------------------------------
\vspace*{0.5cm} 
\begin{flushright} 
	{\huge \textbf{۲۱. \hspace{0.2cm} طراحی نرم‌افزار حساس به ارزش}}
	\addcontentsline{toc}{section}{۲۱. طراحی نرم‌افزار حساس به ارزش}
\end{flushright}

% -----------------------------------------------------------------
% نام نویسنده
% -----------------------------------------------------------------
\vspace{1.5cm} 

\begin{center}
	\Large \textbf{پائولان کورنهوف} \\
	\vspace{0.3cm} 
	\large \lr{(Paulan Korenhof)}
\end{center}

\vspace{1.5cm} 

% -----------------------------------------------------------------
% فهرست مطالب بخش (با ارجاعات اصلاح شده)
% -----------------------------------------------------------------
\noindent
\textbf{\large محتویات بخش}
\vspace{0.5cm}

\noindent
\textbf{۲۱.۱ مقدمه} \dotfill \pageref{sec:21-intro-main} \par
\vspace{0.3cm}

\noindent
\textbf{۲۱.۲ خوب، بد، و «هرگز خنثی»} \dotfill \pageref{sec:21-good-bad-main} \par
\vspace{0.1cm}
\noindent \hspace{0.8cm} ۲۱.۲.۱ عدم خنثی بودن \dotfill \par
\noindent \hspace{0.8cm} ۲۱.۲.۲ تأثیر در سطح خرد \dotfill \par
\noindent \hspace{0.8cm} ۲۱.۲.۳ تأثیر در سطح کلان \dotfill \par
\noindent \hspace{0.8cm} ۲۱.۲.۴ در مجموع \dotfill \par
\vspace{0.3cm}

\noindent
\textbf{۲۱.۳ به‌کارگیریِ «هرگز خنثی»} \dotfill \pageref{sec:21-employing-main} \par
\vspace{0.1cm}
\noindent \hspace{0.8cm} ۲۱.۳.۱ چالشی برای طراحان \dotfill \par
\noindent \hspace{0.8cm} ۲۱.۳.۲ طراحی حساس به ارزش \lr{(VSD)} \dotfill \par
\noindent \hspace{0.8cm} ۲۱.۳.۳ ارزش‌ها \dotfill \par
\noindent \hspace{0.8cm} ۲۱.۳.۴ ارزش‌های قانونی و طراحی \dotfill \par
\vspace{0.3cm}

\noindent
\textbf{منابع} \dotfill \pageref{sec:21-references-main} \par

\vspace{1cm}

% -----------------------------------------------------------------
% کادر اهداف یادگیری
% -----------------------------------------------------------------
\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\begin{center}
			\textbf{\large اهداف یادگیری}
		\end{center}
		\vspace{0.2cm}
		\begin{itemize}
			\item درک اینکه چرا فناوری یک ابزار خنثی نیست.
			\item توانایی تشخیص تأثیر غیرخنثیِ یک فناوری خاص.
			\item تفکر درباره چگونگی تعبیه ارزش‌های اخلاقی در طراحی نرم‌افزار.
		\end{itemize}
		\vspace{0.2cm}
	\end{minipage}
}
\vspace{1cm}

% =================================================================
% شروع متن اصلی
% =================================================================
\clearpage

\subsection{مقدمه} 
\label{sec:21-intro-main}

نرم‌افزار تقریباً در همه جنبه‌های زندگی روزمره ما دخیل است: در دسکتاپ‌ها، لپ‌تاپ‌ها و گوشی‌های هوشمند ما وجود دارد، اما ما همچنین شاهد پیاده‌سازی آن در طیف فزاینده‌ای از اقلام پرمصرف مانند خودروها، دوچرخه‌ها، مسواک‌های برقی، اجاق‌گازها، دستگاه‌های تناسب اندام و اسباب‌بازی‌ها هستیم. ما از نرم‌افزار برای پرداخت، ارتباط، خرید آنلاین، برنامه‌ریزی مسیر، برنامه‌ریزی حمل‌ونقل عمومی، تماشای سریال و فیلم، تصمیم‌گیری در مورد اینکه کدام بیمه را بگیریم، به کدام حزب سیاسی رأی دهیم، سفارش غذا، چک کردن سلامتی‌مان و غیره استفاده می‌کنیم.

و این فقط ما به عنوان شهروندان خصوصی نیستیم که با کمال میل از نرم‌افزار برای بسیاری از امور در فعالیت‌های روزانه خود استفاده می‌کنیم. نهادهای دولتی و کسب‌وکارها نیز برای انجام بسیاری از فرآیندهای خود به‌شدت به نرم‌افزار متکی هستند. آن‌ها گاهی اوقات حتی فرآیندهای تصمیم‌گیری خاصی را کاملاً خودکار می‌کنند، مانند تصمیم‌گیری در مورد اینکه آیا کسی باید برای سرعت غیرمجاز جریمه شود، آیا به کسی باید وام یا کارت اعتباری داده شود، یا اینکه آیا کسی یک متقاضی شغلی امیدوارکننده است یا خیر.

در حالی که استفاده از نرم‌افزار به عنوان ابزاری برای کمک به ما در انواع وظایف مزایای زیادی دارد، اما یک نکته مهم (یا یک «گیر») وجود دارد. آن نکته در مورد برنامه‌های نرم‌افزاری این است که مانند تمام فناوری‌ها، آن‌ها **ذاتاً خنثی نیستند**: هر فناوری دارای یک سوگیری خاص است، شیوه‌ای خاص که در آن احتمالاً بر اعمال ما، انتخاب‌های ما، ادراک ما و نحوه تفسیر ما از جهان اطرافمان تأثیر می‌گذارد. در همین حال، تأثیر نرم‌افزار بر زندگی افراد می‌تواند بسیار زیاد باشد، به‌ویژه که به‌طور فزاینده‌ای عناصر بیشتری از زندگی ما به این برنامه‌ها وابسته و با آن‌ها درهم‌تنیده می‌شوند.

هدف این فصل جلب توجه خوانندگان به این ماهیت غیرخنثیِ فناوری و تشویق آن‌ها به تلاش برای بهره‌برداری از این عدم خنثی بودن به شیوه‌ای سودمند است. این فصل با بحث در مورد اینکه چرا فناوری هرگز یک ابزار خنثی نیست، آغاز می‌شود. با کمک مثال‌های مختلف مربوط به نرم‌افزار، تأثیر فناوری بر عناصر مختلف زندگی انسان مورد بحث قرار می‌گیرد. با توجه به ماهیت ذاتاً غیرخنثیِ فناوری و تأثیرات مشکل‌ساز بالقوه آن، مهم است که بفهمیم چگونه می‌توانیم از ثمرات فناوری بهره‌مند شویم و در عین حال آسیب‌های احتمالی آن را کاهش دهیم.

بنابراین، نیمه دوم فصل استدلال می‌کند که به‌طور ایده‌آل، ما باید از همان ابتدای طراحی فناوری، فعالانه با این عدم خنثی بودن برخورد کنیم. برای کمک به طراحان در این امر، این فصل ایده‌های اصلی زیربنای «طراحی حساس به ارزش» \lr{(VSD)} را معرفی می‌کند. از آنجا که ارائه یک دفترچه راهنمای کامل برای طراحی نرم‌افزار حساس به ارزش در اینجا ممکن نیست، هدف این فصل ارائه «خوراک فکری» کافی به خوانندگان است تا بتوانند خودشان در این سفرِ پیگیری گام بردارند.

% =================================================================
% بخش ۲۱.۲
% =================================================================

\raggedbottom 

\subsection*{۲۱.۲ خوب، بد، و «هرگز خنثی»}
\addcontentsline{toc}{subsection}{۲۱.۲ خوب، بد، و «هرگز خنثی»}
\label{sec:21-good-bad-main}

در این بخش، ما عمیقاً به موضوع عدم خنثی بودن فناوری خواهیم پرداخت. ابتدا، پیشینه دیدگاه عدم خنثی بودن مورد بحث قرار خواهد گرفت. پس از آن، این عدم خنثی بودن با جزئیات بیشتر و با رویکرد به فناوری از دو دیدگاه سطح خرد \lr{(micro-level)} و سطح کلان \lr{(macro-level)} توضیح داده خواهد شد.

\subsubsection*{۲۱.۲.۱ عدم خنثی بودن}
\addcontentsline{toc}{subsubsection}{۲۱.۲.۱ عدم خنثی بودن}
\label{sec:21-non-neutrality}

اهمیت فناوری برای زندگی انسان را به سختی می‌توان نادیده گرفت: جامعه و زندگی آن‌گونه که امروز می‌شناسیم، بدون توسعه و استفاده از فناوری وجود نداشت. فناوری به ما اجازه می‌دهد تا به اهداف خاصی دست یابیم، کارهایی را انجام دهیم و چیزهایی را درک کنیم که بدون استفاده از فناوری قادر به انجام آن‌ها نبودیم، و جهان را به روش‌های جدیدی برای ما آشکار می‌کند.

برای مثال، ما می‌توانیم تک‌سلول‌های بدن را از طریق میکروسکوپ ببینیم، با افرادی در آن سوی کره زمین از طریق تلفن مشورت کنیم، یا با دستگاه اکو به درون بدن نگاه کنیم. با فراهم کردن چنین تجربیات، اقدامات و ادراکات جدیدی از جهان، فناوری ما را قادر می‌سازد تا به روش‌های جدیدی با جهان ارتباط برقرار کنیم و بر تفسیر ما از جهان اطرافمان، و همچنین بر اعمال و قراردادهای اجتماعی ما تأثیر می‌گذارد \lr{(Kiran \& Verbeek, 2010; Verbeek, 2011)}.

به دلیل تأثیر شکل‌دهنده فناوری بر ادراک، تجربه، اعمال، اهداف و درک ما، فناوری از نقشِ صرفاً ابزار بودن فراتر می‌رود. در قرن گذشته، فیلسوفان تکنولوژی استدلال کردند که فناوری **ذاتاً خنثی نیست**: فناوری‌ها می‌توانند جهان را به روش‌های جدیدی برای ما آشکار کنند؛ انتخاب‌ها و امکانات جدیدی برای عمل ایجاد کنند؛ هویت‌های اجتماعی، روابط قدرت، و موقعیت‌های شمول و طرد را ایجاد نمایند؛ و بر ما، انتخاب‌های ما، فرهنگ ما و جهان‌بینی ما تأثیر بگذارند \lr{(see, e.g., Heidegger, 1954; Ihde, 1983; Latour, 1993; Feenberg, 2002; Verbeek, 2005)}. به دلیل این عدم خنثی بودن، فناوری دارای یک تأثیر هنجاری بر رابطه بین انسان‌ها و جهان آن‌هاست \lr{(Hildebrandt, 2015)}.

تحلیل تأثیر و معنای فناوری برای وجود انسان، منجر به پیدایش مکاتب فکری مختلفی در فلسفه تکنولوژی شد. به جای اینکه خواننده را درگیر بحث بین ایده‌های گوناگون کنیم، برای هدف این فصل ارزشمندتر خواهد بود که دو جهت‌گیری اصلیِ این دیدگاه‌ها را به صورت ساده‌سازی شده در نظر بگیریم و آن‌ها را مکمل یکدیگر بدانیم.

به زبان ساده، فناوری بر نحوه تعامل انسان‌ها با جهان تأثیر می‌گذارد که از یک **سطح خرد** (فردی و تجربی) \lr{(see, e.g., Ihde, 1983; Verbeek, 2005)}، تا یک **سطح کلان** (اجتماعی و انتزاعی) \lr{(see, e.g., Stiegler, 1998; Feenberg, 2002)} امتداد دارد.

در حالی که در نظر گرفتن تأثیر فناوری در سطح کلان برای درک دامنه و عمق تأثیر آن ضروری است، تحلیل متمرکز بر سطح خرد می‌تواند برای ردیابی مشکلات به ویژگی‌های ملموس فناوری بسیار مفید باشد. با این حال، من استدلال می‌کنم که سطح خرد و کلانِ تأثیر را نمی‌توان کاملاً از هم جدا کرد، زیرا مکانیسم‌های خرد به تأثیر کلان شکل می‌دهند و بالعکس. با وجود این، برای شفافیت ساختاری، با تمرکز بر سطح خرد شروع می‌کنم و سپس به سطح کلان می‌روم—اما خوانندگان باید توجه داشته باشند که این دو به هم پیوسته هستند.

\subsubsection*{۲۱.۲.۲ تأثیر در سطح خرد}
\addcontentsline{toc}{subsubsection}{۲۱.۲.۲ تأثیر در سطح خرد}
\label{sec:21-micro-level}

در سطح خرد، فناوری با اجازه دادن به ما برای تجربه جهان با واسطه‌گری فناوری، بر ادراک، اعمال، رویه‌ها و اهداف انسانی تأثیر می‌گذارد \lr{(see, e.g., Ihde, 1983; Verbeek, 2005)}.

برای مثال، یک دماسنج می‌تواند دمای بدن ما را نشان دهد. اگر دماسنج عدد $38.5^\circ C$ را نشان دهد، ما احتمالاً نتیجه می‌گیریم که تب داریم، حتی اگر احساس بیماری نکنیم. با گفتن اینکه ما در واقع بیمار هستیم (در حالی که ممکن است احساس خوبی داشته باشیم)، فناوری بر نحوه درک ما از سلامتی‌مان تأثیر می‌گذارد. با انجام این کار، فناوری رابطه ما با جهان (در این مورد، بدن انسان) را «هم‌شکل» \lr{(co-shapes)} می‌دهد.

فناوری بر ادراک ما، اعمال ما و حتی نحوه تفکر و حافظه ما تأثیر می‌گذارد. مورد آخر به وضوح توسط تحقیقات در مورد تأثیرات موتورهای جستجو بر حافظه نشان داده شد: وقتی مردم می‌دانند که می‌توانند برای اطلاعات به موتور جستجو تکیه کنند، تمایل دارند به جای محتوا، «مکان» و «چگونگی» یافتن آن را به خاطر بسپارند \lr{(Sparrow et al., 2011, p. 778)}.

هنگامی که فناوری رابطه ما با جهان را میانجی‌گری می‌کند، عموماً تمرکز خاصی دارد: اغلب جنبه‌های خاصی از واقعیت را آشکار و برجسته می‌کند، در حالی که عناصر دیگر پنهان یا نادیده گرفته می‌شوند \lr{(Verbeek, 2005, p. 131)}. به تماس تلفنی فکر کنید: صدای تماس‌گیرنده برجسته می‌شود، در حالی که بقیه وجودِ فرد پنهان می‌ماند. فناوری بدین ترتیب رابطه خاصی بین انسان و جهان برقرار می‌کند؛ رابطه‌ای که جهت‌دار است. بنابراین می‌توانیم بگوییم که فناوری دارای نوعی «جهت‌مندی» \lr{(directionality)} است \lr{(Verbeek, 2005, p. 115)}.

این جهت‌مندی در طراحی مادی فناوری تعبیه شده است و یک «موضع» \lr{(stance)} خاص می‌گیرد: می‌تواند «پیشنهاد دهد، فعال کند، درخواست کند، ترغیب کند، تشویق کند و برخی اعمال را ممنوع یا ترویج کند» \lr{(Lazzarato \& Jordan, 2014, p. 30)}. یک طراح عموماً با القای ویژگی‌های خاص، قصد دارد جهت‌مندی خاصی به فناوری بدهد. مثلاً فروشگاه‌های آنلاین معمولاً ثبت سفارش را بدون پذیرش «شرایط و ضوابط» غیرممکن می‌سازند. با این حال، نفوذ طراح محدود است و فناوری وجود مستقلی دارد \lr{(Chabot, 2013, p. 15)} و ممکن است اثرات ناخواسته‌ای داشته باشد. اما چون طراحان ویژگی‌های مادی را تعیین می‌کنند، نقش محوری در شکل‌دهی به جهت‌مندی غیرخنثی دارند.

جهت‌مندی نرم‌افزار به ویژگی‌های فناوری و انتخاب‌های طراح بستگی دارد. دانش و محدودیت‌های توسعه‌دهنده پس‌زمینه طراحی را تشکیل می‌دهند \lr{(Kitchin, 2017, p. 18)}. این در یک فرآیند دو مرحله‌ای شکل می‌گیرد: توسعه‌دهنده باید (۱) وظیفه را تفسیر کند و (۲) آن را به کد ترجمه نماید. در این فرآیند، فرضیات و سوگیری‌های طراح در نرم‌افزار گنجانده می‌شود \lr{(Friedman \& Nissenbaum, 1996; Goldman, 2008)}. بنابراین، رویه‌های کدنویسی شده ناگزیر «آکنده از ارزش» هستند \lr{(see, e.g., Brey \& Søraker, 2009; Mittelstadt et al., 2016)}.

\vspace{0.4cm}

\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{مثال ۱} \\
		\textit{تصور کنید شرکتی شما را استخدام می‌کند تا یک آگهی استخدام برای راننده کامیون پخش کنید و تعدادی نامزد بالقوه را انتخاب نمایید. شما تصمیم می‌گیرید یک فرم آنلاین برای روند درخواست شغل ایجاد کنید. برای درخواست، متقاضیان باید نام و تاریخ تولد خود را پر کنند، یک رزومه آپلود کنند، یک گزینه را برای زن یا مرد بودن تیک بزنند، و گزینه‌ای را مبنی بر رضایت به پردازش داده‌های شخصی‌شان تیک بزنند. برای جلوگیری از فراموش کردن موارد توسط افراد، تمام فیلدها را اجباری می‌کنید.}
		
		\par \vspace{0.2cm}
		
		در حالی که چنین فرمی ساده به نظر می‌رسد، در همین کاربرد کوچک نرم‌افزار «جهت‌مندی» خاصی دارد که ممکن است مشکل‌ساز باشد.
		اولاً، آنلاین بودن فرم بلافاصله فرآیند را دیجیتالی می‌کند و ممکن است افراد با مهارت دیجیتال کم را حذف کند.
		
		دوماً، اجباری بودن فرم افراد را مجبور به افشای اطلاعات (یا دروغ گفتن) و شناسایی خود بر اساس گزینه‌های فرم می‌کند. فرم دیدگاهی خاص از جهان دارد: عناصر خاصی را مهم می‌داند و دیدگاه جنسیتی دوگانه (باینری) را بیان می‌کند. متقاضیان ممکن است این «تطبیق هویت با جعبه‌های نرم‌افزار» را مشکل‌ساز بدانند (مثلاً نگرانی حریم خصوصی برای تاریخ تولد، یا عدم تمایل به تعیین جنسیت). اما انتخاب‌های آن‌ها محدود به گزینه‌های فرم آنلاین است.
		\vspace{0.2cm}
	\end{minipage}
}
\vspace{0.4cm}

شکل دادن به نرم‌افزار می‌تواند به ویژه هنگام طراحی نرم‌افزاری که نیاز به تولید تصمیمات بر اساس قوانین خاص دارد (مثل جریمه‌های خودکار سرعت یا یارانه مراقبت از کودک) دشوار باشد. در چنین مواردی، برنامه‌نویس باید قوانین حقوقی یا سیاست‌گذاری را به کد ترجمه کند و ممکن است با سوالاتی مواجه شود که دقیقاً چه زمانی یک مورد خاص تحت تعاریف قانون قرار می‌گیرد. با شکل دادن به چنین مرزهایی در کد، برنامه‌نویس مفاهیم قانونی را پر می‌کند و عملاً یک قاعده سیاستی را تثبیت می‌کند.

نقش انتخاب‌های طراحی در نرم‌افزار را به سختی می‌توان نادیده گرفت: کد کنترل می‌کند کاربران چه کاری می‌توانند و چه کاری نمی‌توانند انجام دهند. این همان چیزی است که لسیگ با بیانیه معروف خود «کد قانون است» منظور داشت \lr{(Lessig, 2006)}. با این حال، همان‌طور که پارایزر توضیح می‌دهد، کد نرم‌افزار با قدرت بیشتری نسبت به قانون رفتار کاربر را کنترل می‌کند، حداقل در ابتدا:

\begin{quote}
	\small 
	«اگر کد قانون است، مهندسان نرم‌افزار و خوره‌های کامپیوتر کسانی هستند که آن را می‌نویسند. و این نوع عجیبی از قانون است که بدون هیچ سیستم قضایی ایجاد شده و فوراً اجرا می‌شود. حتی با وجود قوانین ضد خرابکاری، در دنیای فیزیکی شما هنوز می‌توانید سنگی را به پنجره فروشگاهی که دوست ندارید پرتاب کنید. اما اگر خرابکاری بخشی از طراحی یک دنیای آنلاین نباشد، به سادگی غیرممکن است. سعی کنید سنگی را به یک ویترین مجازی پرتاب کنید؛ شما فقط یک پیام خطا دریافت می‌کنید» \lr{(Pariser, 2011, pp. 96–97)}.
\end{quote}

بنابراین توسعه‌دهنده از طریق معماری نرم‌افزار قدرت زیادی بر کاربران دارد. در این میان، **رابط کاربری** \lr{(interface)} نقش کلیدی ایفا می‌کند؛ از یک سو با پیشنهاد دادن به کاربر که نرم‌افزار چه کاری انجام می‌دهد و از سوی دیگر به عنوان قلمرو تعامل. رابط کاربری ادراک کاربران را شکل می‌دهد، دانش کار با نرم‌افزار را فراهم می‌کند و اقدامات آن‌ها را محدود می‌کند. معمولاً رابط گرافیکی \lr{(GUI)} کد منبع را پنهان می‌کند و عملکرد واقعی نرم‌افزار را غیرشفاف می‌سازد. علاوه بر این، رابط کاربری می‌تواند برای دستکاری یا «تلنگر» \lr{(nudging)} کاربران طراحی شود \lr{(see, e.g., Fogg, 1999; Thaler \& Sunstein, 2009)}. مثلاً استفاده از دکمه سبز بزرگ برای «پذیرش همه کوکی‌ها» در مقابل دکمه قرمز کوچک برای تنظیمات دیگر (به فصل گلرت در همین کتاب مراجعه کنید).

بسته به رابط کاربری، میزان بینش و انتخاب‌های کاربران تغییر می‌کند. این موضوع بر **خودمختاری** \lr{(autonomy)} کاربران تأثیر می‌گذارد: توانایی آن‌ها برای خود-حکمرانی که مستلزم آزادی تصمیم‌گیری آگاهانه است.\footnote{تعریف دقیق اینکه خودمختاری شامل چه چیزی است، بسته به دیدگاه اجتماعی و سیاسی متفاوت است. برای اهداف این فصل، من این مفهوم را نسبتاً باز نگه داشتم تا در رابطه با طراحی نرم‌افزار قابل استفاده باشد.}

برای مثال، برخی فروشگاه‌ها کاربران را ملزم به انتخاب جنسیت «زن» یا «مرد» می‌کنند، برخی گزینه «ترجیح می‌دهم نگویم» دارند و برخی اصلاً آن را اجباری نمی‌کنند. هرچه کاربران آزادانه‌تر انتخاب کنند، خودمختاری بیشتری دارند. کاهش خودمختاری و اجبار کاربران به مسیرهای خاص، می‌تواند آن‌ها را از وظیفه‌شان بیگانه کند. بنابراین دی مول و ون دن برگ استدلال می‌کنند: «آگاهی از، و بینش نسبت به "ماهیت متنی/اسکریپتی" مصنوع، و داشتن توانایی تأثیرگذاری بر آن، برای کاربران در پرتوِ واگذاری خودمختاری‌شان حیاتی است» \lr{(De Mul \& van den Berg, 2011, pp. 59–60)}.

\subsubsection*{۲۱.۲.۳ تأثیر در سطح کلان}
\addcontentsline{toc}{subsubsection}{۲۱.۲.۳ تأثیر در سطح کلان}
\label{sec:21-macro-level}

فناوری نه تنها بر فرآیندها، رویه‌ها و ادراک در سطح فردی تأثیر می‌گذارد، بلکه زندگی ما را در **سطح کلان** نیز تحت تأثیر قرار می‌دهد: سازماندهی و تراکنش‌های اجتماعی، نهادها، عاملیت دولتی، سیاست، علم، روابط بین افراد و حتی هویت ما را تحت نفوذ قرار داده و حتی شکل می‌دهد \lr{(Stiegler, 1998)}.

به‌ویژه در سطح عملکرد شرکت‌ها و نهادها، و همچنین کار افراد درون آن‌ها، استفاده از نرم‌افزار عمیقاً بر فرآیندها و خروجی‌ها تأثیر می‌گذارد که این امر به نوبه خود می‌تواند بر افراد خارج از سازمان و حتی کل جامعه تأثیر بگذارد.

برای مثال، استفاده از نرم‌افزارهای تصمیم‌گیری خودکار، مانند صدور خودکار جریمه برای سرعت غیرمجاز را در نظر بگیرید. این مستلزم تغییری در قدرت تصمیم‌گیری است، که در آن «تصمیم‌گیرنده» اصلی از یک عامل انسانی (که یاد گرفته بود دانش خود از قوانین حقوقی و سیاست‌ها را برای انجام یک ارزیابی بافتی/زمینه‌ای به کار گیرد)، به نرم‌افزاری تغییر می‌کند که قوانین را به شدت و بدون انعطاف اعمال می‌کند:

\begin{quote}
	\small 
	«تصمیمات در الگوریتم‌هایی پیش‌برنامه‌ریزی شده‌اند که همان تدابیر و قوانین را صرف‌نظر از شخص یا بافت اعمال می‌کنند (مثلاً، یک دوربین سرعت‌سنج به بافت [شرایط محیطی] اهمیت نمی‌دهد). مسئولیت تصمیمات اتخاذ شده، در این موارد، از "دیوان‌سالارانِ سطح خیابان" به "دیوان‌سالارانِ سطح سیستم"، مانند مدیران و کارشناسان کامپیوتر، منتقل شده است که تصمیم می‌گیرند چگونه چارچوب‌های سیاستی و قانونی را به الگوریتم‌ها و درخت‌های تصمیم تبدیل کنند» \lr{(Noorman, 2020)}.
\end{quote}

با تغییر قدرت تصمیم‌گیری، چنین نرم‌افزارهایی عموماً فضای اختیار فردی را کاهش می‌دهند و منجر به ایجاد نیروی کاری می‌شوند که تصمیمات را در یک فرآیند تولید یکنواخت، که تأثیر کمی بر آن دارند، تولید انبوه می‌کنند \lr{(Giritli Nygren, 2009; Wihlborg et al., 2016)}. به این ترتیب، نرم‌افزار «روابط، مسئولیت‌ها و صلاحیت‌ها را بازتعریف می‌کند» \lr{(Wihlborg et al., 2016, p. 2903)}.

علاوه بر این، هنگامی که دانش فنی \lr{(know-how)} در نرم‌افزار تعبیه می‌شود، نیاز عملیِ عوامل انسانی به داشتن همان دانش فنی کاهش می‌یابد و گاهی حتی ناپدید می‌شود: یک کلیک روی دکمه می‌تواند برای فراهم کردن آنچه کاربران نیاز دارند کافی باشد. مثالی از این مورد، بانکی است که در آن «مشاوران مشتری نرخ‌های بهره از پیش تعیین‌شده را از سیستم IT برای اعتبار مشتریانشان دریافت می‌کنند، اما نمی‌دانند این نرخ بهره چگونه محاسبه می‌شود یا چه چیزی آن را توجیه می‌کند» \lr{(Spiekermann, 2015, p. 12)}.

با واگذاری دانش فنی به نرم‌افزار، عوامل انسانی و جامعه به طور کلی، به‌طور فزاینده‌ای برای بسیاری از فرآیندهای خود به نرم‌افزار وابسته می‌شوند. استیگلر \lr{(Stiegler)} بنابراین استدلال می‌کند که فناوری به نوعی یک زهر است که هم‌زمان پادزهرِ خودش نیز هست—یک «فارماکون» \lr{(pharmakon)} \lr{(Stiegler, 2012)}: در حالی که نرم‌افزار به انسان‌ها اجازه می‌دهد دانش و چگونگی انجام کارهای خاص را فراموش کنند (زهر)، هم‌زمان با انجام آن اقدامات برای آن‌ها، این از دست دادنِ دانش فنی را جبران می‌کند (پادزهر).

برای مثال به شماره‌های تلفن فکر کنید. در دوران پیش از تلفن همراه، شماره‌های تلفن در تلفن ذخیره نمی‌شدند. این معمولاً بدان معنا بود که شما به‌طور خودکار شماره‌های خانواده و دوستان نزدیک را حفظ می‌کردید زیرا باید مرتباً شماره را تایپ می‌کردید و همچنین، صرف کمی تلاش برای به خاطر سپردن یک شماره سریع‌تر از جستجوی آن در دفترچه تلفن بود. با این حال، با تلفن‌های هوشمند، این نیاز به به خاطر سپردن عملاً زائد شد و تایپ کردن شماره غیرضروری است: فناوری این کار را برای ما انجام می‌دهد. نتیجه این است که ما بسیار کمتر احتمال دارد شماره تلفن‌ها را به خاطر بسپاریم مگر اینکه فعالانه برای حفظ آن‌ها تلاش کنیم. اثر این موضوع زمانی به‌طور دردناکی مشخص می‌شود که دسترسی به لیست مخاطبین در تلفن خود را از دست بدهید.

هرچه بیشتر به نرم‌افزارهای خاص وابسته شویم، قدرت آن‌ها بر ما بیشتر می‌شود. ما می‌توانیم این را به وضوح در استفاده از موتورهای جستجو ببینیم. به دلیل فراوانی منابع اطلاعاتی در وب، ما برای یافتن اطلاعات آنلاین به شدت به استفاده از آن‌ها وابسته شده‌ایم. به این ترتیب، این موقعیت محوریِ موتورهای جستجو، آن‌ها را با قدرت قابل توجهی بر ارتباط بین کاربران و ارائه‌دهندگان محتوا مجهز می‌کند: موتورهای جستجو «لنزهای توجه هستند؛ آن‌ها دنیای آنلاین را در کانون توجه قرار می‌دهند. آن‌ها می‌توانند تغییر مسیر دهند، آشکار کنند، بزرگ‌نمایی کنند و تحریف نمایند. آن‌ها قدرت عظیمی برای کمک کردن و پنهان کردن دارند» \lr{(Grimmelmann, 2010, p. 435)}. حذف شدن از لیست نتایج جستجوی یک موتور جستجو می‌تواند محتوا را برای بخش قابل توجهی از کاربران وب تقریباً نامرئی کند—با تمام عواقب ناشی از آن برای صاحب محتوای وب و همچنین برای کاربران جستجوگر.

قدرت نرم‌افزار با اعتمادی که مردم به فناوری برای انجام صحیح وظایفش دارند، تقویت می‌شود: مردم تمایل دارند دارای یک «سوگیری اتوماسیون» \lr{(automation bias)} باشند که به موجب آن به خروجی نرم‌افزار بیشتر از ارزیابی خود اعتماد می‌کنند \lr{(see, e.g., Skitka et al., 1999)}. به این ترتیب، آن‌ها ممکن است برای ارزیابی خود یا برای تصمیم‌گیری سریع، بیش از حد به نرم‌افزار تکیه کنند \lr{(Skitka et al., 2000)}. ترکیب تمایل انسان به سوگیری اتوماسیون با یک رابط کاربری غیرشفاف که عینیت یا بی‌طرفیِ عملیات نرم‌افزار را القا می‌کند (در حالی که نرم‌افزار در واقع ناگزیر است حاوی برخی سوگیری‌های عمدی یا غیرعمدی و شاید حتی برخی خطاها باشد)، دستورالعملی برای فاجعه است.

مقیاس پردازشِ فراهم شده توسط نرم‌افزار می‌تواند تأثیر سوگیری‌های آن را به سطحی در گستره‌ی جامعه بزرگ‌نمایی کند. نرم‌افزار ممکن است «تأثیرات بسیار عظیم‌ترِ سوگیری‌های سطح سیستم و نقاط کور را عادی‌سازی کند» \lr{(Gandy, 2010, p. 33)}.

برای مثال رسانه‌های اجتماعی را در نظر بگیرید. این نوع نرم‌افزار با ایجاد استانداردهای جدیدی از آنچه «نرمال» در نظر گرفته می‌شود، منجر به تغییراتی در فرهنگ وب شد \lr{(Van Dijck, 2013; Wittkower, 2014)}. یکی از تغییرات ایجاد شده توسط رسانه‌های اجتماعی، تغییر از ارتباطات آنلاین نسبتاً ناشناس به ارتباطات الگویی است که در آن «افراد به‌طور فزاینده‌ای شناخته شده‌اند و در واقع با میل خود بسیاری از اطلاعات شخصی‌شان را آنلاین به اشتراک می‌گذارند» \lr{(Sparrow et al., 2005, p. 283)}.

نقش محوری در اینجا توسط **تنظیمات پیش‌فرض** \lr{(default settings)} نرم‌افزار ایفا می‌شود. تنظیمات پیش‌فرض استانداردی را برای استفاده از آن تعیین می‌کنند و کاربران با ترجیحات متفاوت را ملزم می‌کنند تا برای تنظیم مجدد پیش‌فرض، زمان و تلاش صرف کنند \lr{(see, e.g., van den Berg \& Leenes, 2013; Acquisti et al., 2015)}. تنظیمات پیش‌فرض بدین ترتیب دیدگاه خاصی از جهان، یک «هنجار»، را با توجه به استفاده از آن بیان می‌کنند.

برای مثال، در ابتدا در فیس‌بوک، تنظیم پیش‌فرض یک حساب کاربری این بود که تمام اطلاعات و پست‌های کاربر به‌صورت عمومی در دسترس باشند. با این تنظیمات پیش‌فرض، فیس‌بوک پیشنهاد می‌کرد که استاندارد این است که به عنوان یک شخصِ حقیقیِ خاص، برای مخاطبانی بالقوه در سراسر جهان در دسترس، قابل دسترسی و قابل شناسایی باشید. علاوه بر این، کاربران تمایل دارند تنظیمات پیش‌فرض را بپذیرند، زیرا «راحت است، و مردم اغلب تنظیمات پیش‌فرض را به عنوان توصیه‌های ضمنی تفسیر می‌کنند» \lr{(Acquisti et al., 2015, p. 512)}. بنابراین تنظیمات پیش‌فرض به‌شدت بر رفتار و هنجارهای کاربر تأثیر می‌گذارد.

در نهایت، خروجی نرم‌افزار می‌تواند بر زندگی افرادی که کاربرانِ نرم‌افزار نیستند—و همچنین بر کل جامعه—تأثیر بگذارد. برای مثال با قرار دادن گروه‌های خاصی از مردم در وضعیت نامطلوب. بیایید با تمرکز بر برنامه‌های تصمیم‌گیری خودکار، مانند آن‌هایی که جریمه سرعت صادر می‌کنند، افراد را به عنوان ریسک کلاهبرداری علامت‌گذاری می‌کنند، یا مبلغی را که افراد باید برای بیمه خود بپردازند محاسبه می‌کنند، کمی عمیق‌تر به این موضوع بپردازیم.

در این موارد، افرادی که کاربران اولیه نرم‌افزار نیستند، در معرض خروجی (تصمیم) تولید شده توسط نرم‌افزار قرار می‌گیرند. با این حال، از آنجا که شفافیت خروجی نرم‌افزار وابسته به آن چیزی است که در نرم‌افزار برنامه‌نویسی شده تا به عنوان خروجی نشان داده شود، مردم ممکن است پروفایل‌بندی شوند و در معرض تصمیمی قرار گیرند که چگونگی، چرایی و چیستی آن برایشان روشن نیست. به این ترتیب، برای آن‌ها دشوار است که بفهمند آیا خطایی رخ داده است یا خیر، و اگر بله، کجا و چگونه.

کمبود بینش در مورد آنچه در نرم‌افزار اتفاق می‌افتد می‌تواند به‌ویژه در مورد نرم‌افزارهای تصمیم‌گیری خودکار مورد استفاده توسط آژانس‌های دولتی مشکل‌ساز باشد، زیرا این آژانس‌ها وظیفه دارند در تصمیمات خود شفاف باشند و برای آن‌ها دلیل بیاورند. علاوه بر این، فقدان شفافیت و عدم دسترسی به همان نرم‌افزار، به چالش کشیدنِ مؤثرِ یک تصمیمِ تولید شده به صورت خودکار را برای مردم دشوار می‌کند.

این امر با ایجاد «نابرابری در سلاح‌ها» \lr{(inequality of arms)} بین یک شهروند عادی و آژانس—که عموماً از قبل دارای موقعیت قدرت است زیرا مردم برای یک چیز یا چیز دیگر به آژانس وابسته هستند—منجر به عدم توازن قدرت می‌شود. این‌ها تنها برخی از مسائل مربوط به نرم‌افزارهای تصمیم‌گیری خودکار هستند. تأثیر نرم‌افزارهای تصمیم‌گیری خودکار بر زندگی و جهان ما بحثی بسیار گسترده است که در اینجا نمی‌گنجد.

\subsubsection*{۲۱.۲.۴ در مجموع}
\addcontentsline{toc}{subsubsection}{۲۱.۲.۴ در مجموع}
\label{sec:21-in-sum}

این بخش بحث کرد که چرا نرم‌افزار، مانند تمام فناوری‌ها، یک ابزار خنثی نیست. نرم‌افزار دارای «جهت‌مندی» \lr{(directionality)} خاصی است که به احتمال زیاد بر شیوه‌ای که ما کار می‌کنیم، تصمیم می‌گیریم و تعامل داریم، تأثیر می‌گذارد و حتی آن را تغییر می‌دهد.

تأثیر آن می‌تواند تا عمق جامعه و به‌ویژه به زندگی افراد کشیده شود.\footnote{تحت فشار نهادهای عمومی و قانون‌گذاری اروپا، فیس‌بوک در نهایت تنظیمات پیش‌فرض خود را به مخاطبان محدود تغییر داد و با آن استانداردی تا حدودی دوست‌دارِ حریم خصوصی‌تر تعیین کرد.} سوال اکنون این است که چگونه باید با این عدم خنثی بودن برخورد کنیم.

% =================================================================
% بخش ۲۱.۳ (با لیبل اصلاح شده برای رفع خطا)
% =================================================================

\raggedbottom 

\subsection*{۲۱.۳ به‌کارگیریِ «هرگز خنثی»}
\addcontentsline{toc}{subsection}{۲۱.۳ به‌کارگیریِ «هرگز خنثی»}
\label{sec:21-employing-main} % <--- اصلاح شد: نام لیبل تغییر کرد

این بخش رویکردی را در مورد نحوه برخورد با عدم خنثی بودن فناوری ارائه می‌دهد. این بخش با استدلال برای یک رویکرد پیش‌دستانه \lr{(proactive)} نسبت به ارزش‌ها در طراحی فناوری آغاز خواهد شد. برای ارائه دستگیره‌هایی در مورد چگونگی شروع، این بخش سپس طرح کلی از «طراحی حساس به ارزش» را ارائه می‌دهد. در نهایت، با توجه به تمرکز این فصل بر نرم‌افزار، این بخش نگاهی به ارزش‌های ترویج شده توسط مقررات عمومی حفاظت از داده‌ها \lr{(GDPR)} در هنگام پردازش داده‌های شخصی می‌اندازد.

\subsubsection*{۲۱.۳.۱ چالشی برای طراحان}
\addcontentsline{toc}{subsubsection}{۲۱.۳.۱ چالشی برای طراحان}
\label{sec:21-challenge}

در حالی که فناوری لزوماً خوب یا بد نیست، اما هرگز خنثی نیست. بنابراین طراحی فناوری نقشی محوری دارد: در این مرحله از فرآیند، بخش قابل توجهی از آنچه یک فناوری خاص انجام می‌دهد و انجام نمی‌دهد (یعنی جهت‌مندی آن) تعیین می‌شود. بنابراین دی مول و ون دن برگ اشاره می‌کنند که علی‌رغم نفوذ شدید فناوری بر ما و جهان ما، «مسئولیت آن جهان و آنچه در آن اتفاق می‌افتد هنوز در دست انسان‌هاست و نه در دست فناوری‌ها. از آنجا که انسان‌ها معماران، طراحان و کاربران فناوری‌ها هستند، به همین دلیل مسئول مخلوقات خود و خروجیِ مخلوقاتشان هستند» \lr{(De Mul \& van den Berg, 2011, p. 46)}.

فناوری توسط ما طراحی می‌شود و در بسیاری از موارد، ما قادر خواهیم بود فناوری را به گونه‌ای طراحی کنیم که بتوانیم تأثیرات مشکل‌ساز آن را کاهش دهیم یا حتی از آن‌ها جلوگیری کنیم.

بنابراین راهی برای مقابله با عدم خنثی بودنِ ذاتیِ فناوری، طراحیِ آگاهانه‌ی فناوری به شیوه‌ای است که از ارزش‌های اجتماعی یا اخلاقی خاصی مانند آزادی، ایمنی و حریم خصوصی حمایت یا آن‌ها را ترویج کند. بنابراین، ما باید در همان فرآیند طراحی بپرسیم که تأثیر بالقوه یک نرم‌افزار چه می‌تواند باشد، و اگر بخواهیم ارزش‌های خاصی را ترویج کنیم (در حالی که از درج سوگیری‌های مشکل‌ساز در فناوری جلوگیری می‌کنیم)، این نرم‌افزار چگونه باید کار کند.

البته، همه اثرات آینده و استفاده‌های ناخواسته قابل پیش‌بینی نیستند (به‌ویژه که زندگی واقعی پیچیده و درهم‌ریخته است، نکته‌ای که کیمولن و تیلور در این کتاب به آن اشاره کرده‌اند)، و از همه چیز نمی‌توان پیشگیری کرد. با این حال، شروع خوب این است که از مراحل اولیه طراحی، آگاهانه ارزش‌های خاصی را پیاده‌سازی کنیم و سعی کنیم نسبت به ارزش‌هایی که ناخودآگاه در فناوری می‌سازیم، آگاه شویم. با این کار، «نوآوری فناورانه می‌تواند به نوآوری مسئولانه تبدیل شود» \lr{(van den Hoven et al., 2015, p. 3)}.

این موضوع مسئولیت فعالی را بر دوش مهندسان می‌گذارد. تمرکز آگاهانه بر ارزش‌های درج شده در طراحی می‌تواند کمک کند تا اطمینان حاصل شود که فناوری نیازهای اجتماعی را برآورده می‌کند و به کاهش خطر اثرات ناخواسته یا مضر کمک می‌کند. این همچنین برای طراحان و مهندسان سودمند است، زیرا ممکن است از آسیب به شهرت آن‌ها (زمانی که مردم فناوری را غیرقابل اعتماد یا مضر بدانند) جلوگیری کند.

علاوه بر این، در برخی موارد، طراحی فناوری به گونه‌ای که ارزش‌های خاصی را ترویج کند، حتی توسط قانون الزامی شده است. قانون مهمی در این زمینه \lr{GDPR} است (برای اطلاعات بیشتر در مورد این مقررات به فصل گلرت در این کتاب مراجعه کنید)، که از عواملی که داده‌های شخصی را پردازش می‌کنند می‌خواهد تا در «حریم خصوصی در طراحی» \lr{(privacy by design)} مشارکت کنند (ماده ۲۵، \lr{GDPR}، که بعداً به آن بازخواهم گشت).

\subsubsection*{۲۱.۳.۲ طراحی حساس به ارزش}
\addcontentsline{toc}{subsubsection}{۲۱.۳.۲ طراحی حساس به ارزش}
\label{sec:21-vsd}

یکی از راه‌هایی که می‌توانیم آگاهانه تلاش کنیم با عدم خنثی بودنِ (مشکل‌ساز یا سودمند) فناوری برخورد کنیم، درگیر شدن در شیوه‌ای از طراحی است که «حساس به ارزش» باشد. رویکردهای متعددی برای در نظر گرفتن صریح ارزش‌های انسانی هنگام طراحی فناوری توسعه یافته‌اند. این رویکردها «حداقل در چهار ادعای کلیدی مشترک هستند: ارزش‌ها می‌توانند در فناوری بیان و تعبیه شوند؛ فناوری‌ها تأثیرات واقعی و گاهی غیرواضح بر کسانی دارند که به‌طور مستقیم و غیرمستقیم تحت تأثیر قرار می‌گیرند؛ تفکر صریح در مورد ارزش‌هایی که در طراحی فنی منتقل می‌شوند از نظر اخلاقی مهم است؛ و ملاحظات ارزشی باید در اوایل فرآیند طراحی فنی آشکار شوند» \lr{(Friedman et al., 2017, p. 65)}. مشهورترینِ این رویکردها، «طراحی حساس به ارزش» \lr{(VSD)} است (برای مرور گسترده، نگاه کنید به \lr{Friedman \& Hendry, 2019}).

ایده کلی \lr{VSD} در اواسط دهه ۱۹۹۰ توسعه یافت \lr{(Friedman et al., 2017, p. 64)}. \lr{VSD} رویکردی به طراحی فناوری است که ارزش‌های انسانی را در کل فرآیند طراحی در نظر می‌گیرد \lr{(Friedman et al., 2008, p. 76)}. ون دن هوون آن را به عنوان «یک ادغام پیش‌دستانه‌ی اخلاق—بارگذاری اولیه‌ی اخلاق—در طراحی، معماری، نیازمندی‌ها، مشخصات، استانداردها، پروتکل‌ها، ساختارهای انگیزشی و ترتیبات نهادی» توصیف می‌کند \lr{(Van den Hoven, 2008, p. 63)}.

\lr{VSD} در حال توسعه مداوم است و ممکن است همیشه باشد (که لزوماً چیز بدی نیست). روش‌شناسی کلی آن هنوز با چالش‌هایی روبروست (نگاه کنید به \lr{Friedman et al., 2017; Winkler \& Spiekermann, 2018})—که تعدادی از آن‌ها در زیر بحث خواهد شد. علی‌رغم چالش‌ها، به‌طور کلی، \lr{VSD} یک رویکرد نسبتاً عملی در مورد طراحی فناوریِ آگاه به ارزش است و می‌تواند برای کسانی که در قلب فرآیند طراحی هستند، ارزش قابل توجهی داشته باشد.

روش‌شناسی \lr{VSD} از جمله بر علوم اجتماعی و تحقیقات تعامل انسان و کامپیوتر تکیه دارد \lr{(Friedman et al., 2017, p. 64)}. روش‌شناسی آن مطالعات تجربی، فنی و مفهومی را ترکیب می‌کند و این‌ها را به شیوه‌ای تکرارپذیر و یکپارچه در طول فرآیند طراحی به کار می‌گیرد \lr{(Friedman et al., 2008, p. 93)}. با این ترکیب روش‌شناختی، \lr{VSD} یک موضع تعاملی اتخاذ می‌کند و از این پیش‌فرض شروع می‌کند که «انسان‌هایی که به عنوان افراد، سازمان‌ها یا جوامع عمل می‌کنند، ابزارها و فناوری‌هایی را که طراحی و پیاده‌سازی می‌کنند شکل می‌دهند؛ و در مقابل، آن ابزارها و فناوری‌ها تجربه انسانی و جامعه را شکل می‌دهند» \lr{(Friedman et al., 2017, p. 68)}.

\vspace{0.4cm}

\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{مثال ۲} \\
		\textit{تصور کنید می‌خواهید نرم‌افزاری توسعه دهید که به مردم کمک کند زمان کمتری را صرف نگاه کردن به گوشی هوشمند خود کنند. با استفاده از تحلیل تجربی، می‌توانید زمینه و تجربه استفاده فعلی افراد از گوشی هوشمند را بررسی کنید و ایده‌ای از خواسته‌ها و مشکلات آن‌ها به دست آورید.}
		
		\par \vspace{0.2cm}
		
		برای بررسی این موضوع، به‌طور ایده‌آل از روش‌های تحقیق کمی و/یا کیفی از علوم اجتماعی، مانند مصاحبه، نظرسنجی و تحلیل آماری استفاده می‌کنید. علاوه بر این، می‌توانید از چنین تحلیل‌های تجربی برای آزمایش طراحی خود استفاده کنید.
		
		با این حال، این تحلیل‌ها تمام آن چیزی نیستند که مرتبط است. کاربران ممکن است همیشه ندانند چه می‌خواهند (به‌ویژه از قبل)، یا از تمام پیامدهای کاری که انجام می‌دهند و استفاده می‌کنند آگاه نباشند، و همچنین ممکن است شما داده‌های کافی برای نظارت بر تصویر بزرگ‌تر نداشته باشید. بنابراین انجام یک **تحلیل مفهومی** \lr{(conceptual analysis)} برای به دست آوردن تصویری کامل(تر) از مفاهیم و مسائل درگیر، مانند ارزش‌هایی که نقش دارند یا پیامدهای گسترده‌تر فردی و اجتماعی فناوری، مهم است.
		
		برای این تحلیل مفهومی، شما از تحقیقات نظری و فلسفی استفاده می‌کنید که به مفاهیم و مسائل اصلی که به نوعی به فناوری (طراحی‌شونده) مربوط می‌شوند، می‌پردازد. بیایید بگوییم برای این نرم‌افزار، شما مطالعاتی در مورد حساب‌های فلسفیِ عاملیت، خودمختاری، تلنگر \lr{(nudging)}، دستکاری و حریم خصوصی خواهید خواند. این می‌تواند به نوبه خود به پرسش‌های تجربی بعدی و طراحی فناورانه شما جهت دهد.
		
		بنابراین انجام یک **تحلیل فنی** \lr{(technological analysis)} از فناوری نیز مهم است. درک بهتر از فناوری می‌تواند با تحلیل مکانیسم‌ها و نتایج ملموس آن، و همچنین با نگاه کردن به فناوری‌های موجود که شباهت‌های خاصی دارند و ارزیابی تأثیر آن‌ها، حاصل شود. یافته‌های شما از مکانیسم‌های فنی می‌تواند تحلیل مفهومی و تجربی شما را بیشتر آگاه و مشخص کند، که به نوبه خود می‌تواند به شما در بهبود طراحی کمک کند. و این فرآیند رفت و برگشت ادامه می‌یابد تا زمانی که به طرحی برسید که جامع باشد و توسط تحقیقات پشتیبانی شود.
		\vspace{0.2cm}
	\end{minipage}
}
\vspace{0.4cm}

\subsubsection*{۲۱.۳.۳ ارزش‌ها}
\addcontentsline{toc}{subsubsection}{۲۱.۳.۳ ارزش‌ها}
\label{sec:21-values}

در زمینه \lr{VSD}، اصطلاح «ارزش» به «آنچه برای مردم در زندگی‌شان مهم است، با تمرکز بر اخلاق و معنویات» اشاره دارد \lr{(Friedman \& Hendry, 2019, p. 24)}. بنابراین تمرکز بر ارزش‌های اجتماعی و اخلاقی است، نه بر ارزش اقتصادی. در این زمینه، می‌توانیم به ارزش‌هایی مانند رفاه انسانی، اعتماد، حریم خصوصی، انصاف، خودمختاری، کاربردپذیری جهانی، ایمنی، سلامت و پایداری محیط زیست فکر کنیم. ارزش‌هایی که به‌طور بالقوه توسط \lr{VSD} پوشش داده می‌شوند، از آن‌هایی که می‌توانند در نظریه‌های مختلف فلسفی اخلاقی مانند وظیفه‌گرایی، پیامدگرایی و اخلاق فضیلت یافت شوند (نگاه کنید به فصل کیمولن و تیلور در این کتاب)، تا ارزش‌های شخصی مانند ترجیحات سلیقه و رنگ، و قراردادهایی مانند استانداردهای پروتکل متغیر است \lr{(Friedman et al., 2008, p. 94)}.

\lr{VSD} تمایل دارد انتخاب و ارزیابی ارزش خود را بر اساس تجربیات و نظرات ذینفعان بنا کند. بنابراین، یک عنصر کلیدی در \lr{VSD} شناسایی ذینفعان مستقیم و غیرمستقیم و ارزش‌های مربوط به آن‌هاست \lr{(Friedman et al., 2017, p. 69)}. ذینفعان مستقیم افراد، گروه‌ها یا سازمان‌هایی هستند که مستقیماً با فناوری مورد نظر تعامل دارند \lr{(Friedman et al., 2017, p. 76)}. ذینفعان غیرمستقیم شامل افراد، گروه‌ها یا سازمان‌هایی هستند که تحت تأثیر فناوری قرار می‌گیرند، اما مستقیماً با آن تعامل ندارند \lr{(Friedman et al., 2017, p. 76)}.

مثالی از روشی برای درک ارزش‌های در معرض خطرِ ذینفعان مستقیم و غیرمستقیم، انجام مصاحبه‌های نیمه‌ساختاریافته است \lr{(Friedman et al., 2008, pp. 100–101)}. با این حال، شناسایی ذینفعان می‌تواند دشوار باشد، و شکست در شناسایی گروه خاصی از ذینفعان می‌تواند منجر به طرد آن‌ها و همچنین طرد ارزش‌های خاص شود \lr{(Manders-Huits, 2011; Winkler \& Spiekermann, 2018)}. علاوه بر این، خود ذینفعان ممکن است همیشه نتوانند بر تأثیر فناوری‌های خاص نظارت کنند و تشخیص دهند که کدام یک از ارزش‌های آن‌ها در یک زمینه خاص ممکن است در خطر باشد.

در برخی موارد، مشخص می‌شود که دو یا چند ارزش متضاد درگیر هستند. این ارزش‌های متضاد لزوماً نباید از ذینفعان مختلف سرچشمه بگیرند: همان ذینفع می‌تواند ارزش‌های متعددی داشته باشد که ممکن است طراحی را به جهات مختلف بکشاند. اگر تنشی بین ارزش‌ها وجود داشته باشد، مهم است که این موضوع را در فرآیند طراحی در نظر بگیریم \lr{(Friedman et al., 2017, p. 69)}.

\vspace{0.4cm}

\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{مثال ۳} \\
		\textit{یک نمونه از تنش بین ارزش‌ها، تنش احتمالی بین حریم خصوصی و امنیت ملی یا عمومی است: در حالی که حریم خصوصی عموماً از جمع‌آوری و افشای داده‌های شخصی کمتر سود می‌برد، امنیت عموماً از دسترسی به داده‌های شخصی بیشتر سود می‌برد.}
		
		\par \vspace{0.2cm}
		
		این تنش نقش محوری در معرفی اسکنرهای بدن در فرودگاه‌ها داشت. هدف اسکنر بدن افزایش ایمنی با نشان دادن بصری به کارکنان امنیتی فرودگاه است که افراد کجای بدنشان اشیاء حمل می‌کنند. برای این کار، آن‌ها سطح بدن را اسکن می‌کنند و این می‌تواند دید نسبتاً دقیقی از آنچه بدن برهنه شخص اسکن شده به نظر می‌رسد نمایش دهد. این کار عمیقاً حریم خصوصی بدنیِ افراد اسکن شده را نقض می‌کند.
		
		بسیاری از توسعه‌دهندگان اسکنر بدن این نقض حریم خصوصی را به عنوان یک قربانیِ قابل قبول به نام ایمنی بدیهی فرض کردند \lr{(Spiekermann, 2015, p. 169)}. با این حال، مشخص شد که نه عموم مردم و نه کارکنان امنیتی و اپراتورهای فرودگاه (که مجبور بودند با شکایات مشتریان برخورد کنند و از کاهش مشتریان می‌ترسیدند) از این نقض حریم خصوصی راضی نبودند.
		
		یک شرکت هر دو ارزش—ایمنی و حریم خصوصی—را جدی گرفت و سعی کرد نرم‌افزار اسکنری طراحی کند که نقض حریم خصوصی را کاهش دهد و در عین حال هدف ایمنی خود را حفظ کند. در طراحی حاصل، نمایش بدن با یک طرح کلی انتزاعی از یک بدن جایگزین شد که در آن مناطقی که شیء روی بدن قرار داشت علامت‌گذاری شده بود. با این طراحی، شرکت به هدف ایمنی خود رسید و در عین حال تدابیر حفاظتی حریم خصوصی را در نرم‌افزار ایجاد کرد. با جدی گرفتن هر دو ارزش و تعبیه آن‌ها در طراحی، شرکت موفق شد اکثریت بازار را تصاحب کند \lr{(Spiekermann, 2015, p. 169)}.
		\vspace{0.2cm}
	\end{minipage}
}
\vspace{0.4cm}

\subsubsection*{۲۱.۳.۴ ارزش‌های قانونی و طراحی}
\addcontentsline{toc}{subsubsection}{۲۱.۳.۴ ارزش‌های قانونی و طراحی}
\label{sec:21-legal-values}

یک منبع خوب برای یافتن ارزش‌هایی که طراحی نرم‌افزار حساس به ارزش باید به‌طور ایده‌آل در نظر بگیرد، قانون است. در زمینه طراحی نرم‌افزار، \lr{GDPR} به دلیل تمرکز بر پردازش داده‌ها از اهمیت ویژه‌ای برخوردار است. \lr{GDPR} مجموعه‌ای از ارزش‌ها را برای ما فراهم می‌کند که باید به نمایندگی از (حفاظت از) «موضوعات داده» \lr{(data subjects)} و کل جامعه در نظر گرفته شوند.

در زیر برخی از ارزش‌های اصلی (مشتق شده) ذکر شده‌اند که می‌توان در \lr{GDPR} یافت (شبیه ساختار تصویر):

\begin{itemize}
	\item[\textbf{--}] \textbf{خودمختاری} \lr{(Autonomy)} (نگاه کنید به، مثلاً، رضایت آگاهانه، ماده ۴(۱۱)، ماده ۷)
	\item[\textbf{--}] \textbf{حریم خصوصی} \lr{(Privacy)} (نگاه کنید به، مثلاً، کنترل بر داده‌های شخصی، مواد ۱۷ و ۲۱)
	\item[\textbf{--}] \textbf{حفاظت در برابر عدم تعادل قدرت} (نگاه کنید به، تصمیم‌گیری فردی خودکار، ماده ۲۲؛ محدودیت هدف، ماده ۵(۱)(ب)؛ به حداقل رساندن داده‌ها)
	\item[\textbf{--}] \textbf{کرامت انسانی} \lr{(Human dignity)}
	\item[\textbf{--}] \textbf{انصاف} \lr{(Fairness)} (نگاه کنید به، ماده ۵(۱)(الف))
	\item[\textbf{--}] \textbf{ایمنی/حفاظت} \lr{(Safety/protection)} (نگاه کنید به، ماده ۲۵)
	\item[\textbf{--}] \textbf{امنیت} \lr{(Security)} (نگاه کنید به، ماده ۳۲)
	\item[\textbf{--}] \textbf{احترام به حقوق و آزادی افراد}
	\item[\textbf{--}] \textbf{رفاه انسانی} \lr{(Human welfare)}
	\item[\textbf{--}] \textbf{شفافیت} \lr{(Transparency)} (نگاه کنید به، ماده ۵(۱)(الف))
	\item[\textbf{--}] \textbf{رفاه اقتصادی} \lr{(Economic prosperity)}
\end{itemize}

آنچه در مورد \lr{GDPR} در پرتو این فصل جالب است، این است که \lr{GDPR} حتی به‌طور صریح تعبیه برخی از ارزش‌های زیربنایی خود را در طراحی نرم‌افزار الزامی می‌کند. ماده ۲۵(۱) \lr{GDPR} در مورد «حفاظت از داده‌ها از طریق طراحی و به‌طور پیش‌فرض» بیان می‌کند:

\begin{quote}
	\small
	«با در نظر گرفتن وضعیت هنر [تکنولوژی روز]، هزینه اجرا و ماهیت، دامنه، زمینه و اهداف پردازش و همچنین خطراتِ با احتمال و شدتِ متغیر برای حقوق و آزادی‌های اشخاص حقیقی که ناشی از پردازش است، کنترل‌کننده باید (...) اقدامات فنی و سازمانی مناسب، مانند نام‌گذاری مستعار \lr{(pseudonymisation)} را اجرا کند که برای پیاده‌سازی اصول حفاظت از داده‌ها، مانند به حداقل رساندن داده‌ها، به شیوه‌ای مؤثر و برای ادغام تدابیر حفاظتی لازم در پردازش به منظور برآوردن الزامات این مقررات و حفاظت از حقوق موضوعات داده طراحی شده‌اند (ماده ۲۵(۱)، \lr{GDPR}).»
\end{quote}

اصول حفاظت از داده‌ها (که در فصل گلرت به‌طور مفصل تحلیل شده‌اند) در ماده ۵ \lr{GDPR} ذکر شده‌اند و بیان می‌کنند که داده‌های شخصی باید:
به‌طور قانونی، منصفانه و شفاف پردازش شوند («قانونی بودن، انصاف و شفافیت»)؛
فقط برای اهداف مشخص، صریح و مشروع جمع‌آوری شوند («محدودیت هدف»)؛
کافی، مرتبط و محدود به آنچه ضروری است باشند («به حداقل رساندن داده‌ها»)؛
دقیق و در صورت لزوم به‌روز باشند («دقت»)؛
به شکلی نگهداری شوند که شناسایی موضوعات داده را بیش از حد لازم ممکن نسازد («محدودیت ذخیره‌سازی»)؛
و به شیوه‌ای پردازش شوند که امنیت مناسب داده‌های شخصی را تضمین کند («یکپارچگی و محرمانگی»).

نقش مهم این اصول، محدود کردن داده‌های شخصی است که می‌توان جمع‌آوری و نگهداری کرد. در اینجا، ضرب‌المثل «دانش قدرت است» به ذهن می‌آید. در این زمینه، اصل محدودیت هدف، اصل به حداقل رساندن داده‌ها و اصل محدودیت ذخیره‌سازی محدودیت‌های مهمی هستند که عدم تعادل قدرتی را که ممکن است بین شهروندان و موسسات/شرکت‌ها ایجاد شود، کاهش می‌دهند.

در سوی دیگر سکه، اقداماتی در \lr{GDPR} وجود دارد که هدف آن‌ها متعادل کردن بهتر زمین بازی با اطمینان از اینکه خودِ موضوعات داده دانش کافی در مورد نحوه پردازش داده‌هایشان دارند، است. به عنوان مثال، ماده ۲۲ \lr{GDPR} به‌ویژه به تصمیم‌گیری خودکار می‌پردازد و در مواردی که تصمیم تولید شده می‌تواند تأثیر قابل توجهی بر زندگی یک فرد داشته باشد، مداخله انسانی را الزامی می‌کند. یک مثال از تأثیر قابل توجه، رد خودکار درخواست اعتبار آنلاین بدون مداخله انسانی است (مقدمه ۷۱). تصمیم‌گیری خودکار «باید تابع تدابیر حفاظتی مناسب باشد، که باید شامل اطلاعات خاص به موضوع داده و حق دریافت مداخله انسانی، بیان دیدگاه خود، دریافت توضیح در مورد تصمیم اتخاذ شده و به چالش کشیدن تصمیم باشد» (مقدمه ۷۱).\footnote{با این حال، الزام حضور یک عامل انسانی در حلقه تصمیم‌گیری تضمین نمی‌کند که ارزش‌های مورد نظر محافظت می‌شوند \lr{(Binns, 2019)}. مداخله انسانی نیز مزایا و معایب خود را دارد: انسان‌ها می‌توانند عمداً و سهواً تبعیض قائل شوند. علاوه بر این، اگر یک انسان به حلقه تصمیم‌گیری اضافه شود، این خطر وجود دارد که انسان صرفاً تصمیمات گرفته شده را برای اعتبارسنجی خروجی آن‌ها و دور زدن الزامات بیشتر ماده ۲۲ \lr{GDPR} «مهر تأیید» بزند \lr{(Veale \& Edwards, 2018, p. 400)}.}

چالش طراحی نرم‌افزار به صورت حساس به ارزش و تلاش برای حساب کردن روی ارزش‌هایی مانند موارد فوق، چگونگی تعبیه ملموس این ارزش‌ها در محصول طراحی شده است. برای این کار، هیچ روش یکسانی برای همه وجود ندارد.

علاوه بر این، خودِ «چگونگی» موضوع تحقیقات در حال انجام است. در مورد طراحی نرم‌افزار به شیوه‌ای که حریم خصوصی را افزایش دهد، مقاله «تحلیل انتقادی استراتژی‌های طراحی حریم خصوصی» توسط کولسکی و همکاران \lr{(Colesky et al., 2016)} می‌تواند منبع الهام ارزشمندی باشد. محققان چندین «استراتژی طراحی حریم خصوصی» را شناسایی می‌کنند که می‌توانند برای تعبیه حریم خصوصی در طراحی استفاده شوند. یکی از تاکتیک‌های پیشنهادی **به حداقل رساندن داده‌ها** است؛ این به ماده ۵(۱)(ج) \lr{GDPR} مربوط می‌شود. این مستلزم انتخابی است که در آن داده‌هایی که مورد نیاز نیستند حذف، جدا یا نابود می‌شوند.

استراتژی‌های دیگر شامل محدودیت دسترسی و تفکیک داده‌ها هستند \lr{(Colesky et al., 2016)}. با ایزوله کردن مجموعه‌های داده، یا با توزیع آن‌ها در مکان‌های مختلف، خطر ترکیب داده‌ها و ارائه دیدگاه دقیق‌تر در مورد یک فرد خاص کاهش می‌یابد. تاکتیک دیگری که آن‌ها ذکر می‌کنند **انتزاع** \lr{(abstraction)} است. اگر داده‌ها در سطح کلی‌تری خلاصه یا گروه‌بندی شوند، تمرکز داده‌ها از افراد خاص به سطح عمومی‌تر تغییر می‌کند. مطالعاتی مانند آنچه توسط کولسکی و همکاران انجام شده است، می‌تواند منبع الهام برای طراحان باشد تا طرح‌هایی ارائه دهند که ارزش‌های مورد نظرشان را محقق سازد.

% =================================================================
% بخش نتیجه‌گیری
% =================================================================

\raggedbottom

\vspace{0.5cm}

\noindent
\fcolorbox{black}{gray!15}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.4cm}
		\begin{center}
			\textbf{\Large نتیجه‌گیری}
		\end{center}
		\vspace{0.3cm}
		
		فناوری خنثی نیست. می‌تواند بر ادراک مردم، آنچه می‌دانند، آنچه می‌توانند انجام دهند، نحوه تعامل آن‌ها با جهان، و شیوه‌ای که جامعه، دولت‌ها، شرکت‌ها و دیگران با آن‌ها تعامل دارند، تأثیر بگذارد و آن را شکل دهد. بنابراین استفاده نوآورانه از فناوری می‌تواند بسیار ارزشمند، اما همچنین بسیار مشکل‌ساز باشد. از این رو، فناوری الهام‌بخشِ هم آرمان‌شهرها \lr{(utopias)} و هم ویران‌شهرها \lr{(dystopias)} بوده است.
		
		طراحی حساس به ارزش رویکردی است که هدف آن مقابله با عدم خنثی بودن فناوری به شیوه‌ای سودمند است. این رویکرد بر هدف‌گذاری فعالانه برای گنجاندن ارزش‌های خاص در طراحی فناوری تمرکز دارد. در حالی که \lr{VSD} بدون مشکلات و چالش‌ها نیست، شروعی امیدوارکننده برای طراحی فناوری است که هدف آن قرار گرفتن در سمت آرمان‌شهریِ امور است تا سمت ویران‌شهری. در برخی موارد، قانون حتی تعبیه ارزش‌های خاصی را در طراحی نرم‌افزار الزامی می‌کند: ماده ۲۵ \lr{GDPR} خواستار اجرای «حریم خصوصی در طراحی» و «حریم خصوصی به‌طور پیش‌فرض» است.
		
		طراحی حساس به ارزش کار آسانی نیست، به‌ویژه به این دلیل که اغلب دشوار، اگر نه غیرممکن، است که تأثیر و استفاده از یک فناوری جدید را به‌طور کامل پیش‌بینی کنیم. با این حال، این نباید ما را از تلاش باز دارد. در اینجا نقش محوری برای طراحان وجود دارد. با آگاهی از دیدگاه‌ها و فرضیاتی که لزوماً در طراحی سیستم ساخته شده‌اند، آن‌ها می‌توانند سعی کنند این کار را به شیوه‌ای آگاهانه و حساس به ارزش انجام دهند.
		
		به منظور شروع طراحی حساس به ارزش، در نظر داشتن **قواعد سرانگشتی** زیر می‌تواند کمک‌کننده باشد:
		
		\begin{enumerate}
			\item \textbf{از عدم خنثی بودنِ ذاتیِ آنچه طراحی می‌کنید آگاه باشید:} فکر کنید که فناوری چه چیزی را به وضعیت فعلی اضافه می‌کند، چه چیزی را از آن می‌گیرد، یا چه چیزی را تغییر می‌دهد.
			\item \textbf{شناسایی کنید که کدام ارزش‌ها را می‌خواهید با طراحی خود تایید کنید} (مثلاً ممکن است بخواهید نرم‌افزاری طراحی کنید تا کارایی را ارتقا دهید و در عین حال از حریم خصوصی محافظت کنید).
			\item \textbf{تأثیر طراحی را ارزیابی کنید:} آیا فناوری به گروه خاصی از مردم سود می‌رساند یا تأثیر منفی می‌گذارد؟ و این افراد چه کسانی هستند و پیامدهای آن برای آن‌ها چیست؟
			\item \textbf{ردیابی کنید که آیا ممکن است تعصبات خاصی را به‌طور غیرضروری یا نامطلوب در طراحی درج کنید} (مثلاً آیا کاربری که در ذهن دارید نماینده کل گروه کاربران است، یا ناخودآگاه نرم‌افزار را به گونه‌ای طراحی می‌کنید که فقط به زیرمجموعه خاصی از کاربران سود می‌رساند؟).
			\item \textbf{سعی کنید ببینید آیا می‌توانید طراحی را تنظیم کنید تا از شر سوگیری ناخواسته یا تأثیر منفی خلاص شوید}، در حالی که ارزش‌هایی را که می‌خواهید تایید کنید ارتقا می‌دهید (یعنی: آزمایش، ارزیابی، تنظیم).
		\end{enumerate}
		
		چگونگی تحقق (بهترین) طراحی حساس به ارزش (و به‌ویژه حریم خصوصی در طراحی و به‌طور پیش‌فرض، با توجه به اینکه این‌ها توسط قانون الزامی شده‌اند) هنوز—و با فناوری‌های جدید همیشه خواهد بود—موضوع کاوش و آزمایش است. با این حال، اولین قدم آگاهی از عدم خنثی بودن فناوری، و تمایل به تفکر در مورد این است که کدام ارزش‌ها به‌طور ایده‌آل باید در طراحی آن محافظت شوند و چگونه می‌توان به این امر دست یافت. امیدواریم این فصل به خوانندگان در برداشتن این قدم اول کمک کند.
		\vspace{0.4cm}
	\end{minipage}
}

% =================================================================
% بخش پرسش و پاسخ
% =================================================================

\raggedbottom 

\vspace{0.8cm}

\noindent
\textbf{\large سوال:} \\
\textbf{طراحان چگونه می‌توانند بر عدم خنثی بودن نرم‌افزار تأثیر بگذارند؟}

\vspace{0.3cm}
\noindent
\textbf{\large پاسخ:} \\
طراحان با طراحی نرم‌افزار برای انجام وظایف خاص و کمک به کاربران برای رسیدن به اهداف معین، عمداً بر جهت‌مندیِ غیرخنثیِ فناوری تأثیر می‌گذارند. آن‌ها تعیین می‌کنند که کاربر با برنامه چه کاری می‌تواند و چه کاری نمی‌تواند انجام دهد و بدین ترتیب بر عدم خنثی بودن در سطح «عمل کاربر» تأثیر می‌گذارند. این موضوع در کد برنامه «تثبیت» \lr{(set in stone)} شده است. علاوه بر این، طراحان تعیین می‌کنند که کاربر هنگام تعامل با برنامه (در ترکیب با ویژگی‌های سخت‌افزار مورد استفاده) چه چیزی را درک می‌کند، و بدین ترتیب تجربه و تفسیر کاربر از فناوری را هدایت می‌کنند.

در اینجا تصویری که طراح از کاربر در ذهن دارد نقش مهمی ایفا می‌کند. یک مثال، طراحی وب‌سایتی با متن کم و تصاویر زیاد است. کاربرانی با اختلال بینایی که برای وب‌گردی خود به برنامه‌ای وابسته هستند که متن را با صدای بلند می‌خواند، برای پیمایش در چنین وب‌سایتی با مشکل مواجه خواهند شد.

علاوه بر این، طراحان می‌توانند با تعبیه فرضیات خود در طراحی، به‌طور ناخودآگاه بر عدم خنثی بودن نرم‌افزار تأثیر بگذارند. یک مثال، فرم آنلاین درخواست کارت اعتباری است که فرد را ملزم می‌کند مستقیماً با تلفن هوشمند از کارت شناسایی عکس بگیرد و اجازه آپلود فایل را نمی‌دهد. این فرض را ایجاد می‌کند که همه کاربران اینترنت دارای تلفن هوشمند هستند.

علاوه بر این، با استفاده از دفترچه‌های راهنمای کاربر و بازاریابی، می‌توان دیدگاه‌های خاصی در مورد فناوری و استفاده از آن را تحت تأثیر قرار داد و ترویج کرد؛ برای مثال با پیشنهاد اینکه استفاده از یک نرم‌افزار خاص می‌تواند سلامت، موقعیت اجتماعی، دوستی و کارایی را بهبود بخشد، خطاهای انسانی را کاهش دهد و غیره.

\vspace{0.8cm}
\hrule
\vspace{0.8cm}

\noindent
\textbf{\large سوال:} \\
\textbf{مزایای طراحی حساس به ارزش چیست؟}

\vspace{0.3cm}
\noindent
\textbf{\large پاسخ:} \\
اول و مهم‌تر از همه، درگیر شدن در طراحی حساس به ارزش به طراحان کمک می‌کند تا با بارگذاریِ اولیه‌ی ارزش‌های اخلاقی در طراحیِ نرم‌افزار (در رابط کاربری، معماری، استانداردها، مشخصات، ساختار انگیزشی، جاسازی نهادی، تنظیمات پیش‌فرض، نیازهای کاربر و غیره) به شیوه‌ای اخلاقاً مسئولانه نوآوری کنند.

علاوه بر این، در نظر گرفتن منافع ذینفعان مختلفِ مستقیم و غیرمستقیم و تلاش برای حساب کردن روی ارزش‌های آن‌ها در طراحی تا حد امکان، به ایجاد حمایت اجتماعی بیشتر برای استفاده از فناوری کمک خواهد کرد. افراد خوشحال‌تر عموماً به معنای کاربران بیشتر و تعامل کاربر بیشتر است.

همچنین، از آنجا که تامل دقیق در مورد تأثیر بالقوه نرم‌افزار بخش ضروریِ طراحیِ حساس به ارزش است، طراحان (یا شرکت سفارش‌دهنده) کمتر احتمال دارد که با پیامدهای پیش‌بینی‌نشده‌ی فناوری غافلگیر شوند.

\vspace{0.8cm}
\hrule
\vspace{0.8cm}

\noindent
\textbf{\large سوال:} \\
\textbf{متن ماده ۲۵(۱) \lr{GDPR} را که در متن بالا ذکر شد دوباره بخوانید. چه چیزی برای تحقق صحیح «حریم خصوصی در طراحی» مهم است؟}

\vspace{0.3cm}
\noindent
\textbf{\large پاسخ:} \\
ماده ۲۵(۱) \lr{GDPR} خواستار اجرای بی‌چون‌ و چرایِ حریم خصوصی در طراحی نیست. در عوض، خواستار ایجاد تعادلی ظریف بین گزینه‌های فنی موجود، منافع افراد درگیر، اهداف پردازش داده‌ها، و زمینه، خطرات و تأثیر آن بر مردم است.

بنابراین تنها حریم خصوصی نیست که باید به عنوان یک ارزش در طراحی در نظر گرفته شود: ارزش‌های دیگر، مانند ایمنی، رفاه انسانی و رفاه اقتصادی نیز باید در نظر گرفته شوند و با حریم خصوصی متعادل شوند.

به زبان ساده، حریم خصوصی در طراحی به معنای پیشگیری فعالانه از هرگونه نقض احتمالی حریم خصوصی است که برای تحقق یک هدف خاص کاملاً ضروری نیست. همان‌طور که مورد اسکنرهای بدن که در بخش ۲۱.۳ بحث شد نشان می‌دهد، یک تعادل دقیق می‌تواند منجر به طرحی شود که قادر است تا حد قابل توجهی به ارزش‌های متضاد احترام بگذارد: اسکنرهای بدنِ «آدمک‌های خطی» در حالی که هدف ایمنی خود را حفظ می‌کنند، نقض حریم خصوصی افراد اسکن شده را نیز به‌طور قابل توجهی کاهش می‌دهند. با این تعادل، این اسکنرهای بدن نمونه خوبی از حریم خصوصی در طراحی هستند.

% =================================================================
% بخش منابع
% =================================================================

\raggedbottom

\vspace{1cm}

\section*{منابع}
\addcontentsline{toc}{section}{منابع}
\label{sec:21-references-main}

\begin{latin}
	\begin{itemize}
		\setlength\itemsep{0.5em}
		
		\item[] Acquisti, A., Brandimarte, L., \& Loewenstein, G. (2015). Privacy and human behavior in the age of information. \textit{Science}, 347(6221), 509–514.
		
		\item[] Binns, R. (2019). Human judgement in algorithmic loops; individual justice and automated decisionmaking. \textit{Individual Justice and Automated Decision-Making} (September 11, 2019).
		
		\item[] Brey, P., \& Søraker, J. H. (2009). Philosophy of computing and information technology. In: \textit{Philosophy of technology and engineering sciences}, Elsevier, pp. 1341–1407.
		
		\item[] Brouwer, E., et al. (2011). Legality and data protection law: The forgotten purpose of purpose limitation.
		
		\item[] Chabot, P. (2013). \textit{The philosophy of Simondon: Between technology and individuation}. A\&C Black.
		
		\item[] Colesky, M., Hoepman, J. H., \& Hillen, C. (2016). A critical analysis of privacy design strategies. In: \textit{2016 IEEE Security and Privacy Workshops (SPW)}, IEEE, pp 33–40.
		
		\item[] De Mul, J., \& van den Berg, B. (2011). Remote control: Human autonomy in the age of computermediated agency. In: \textit{Law, human agency and autonomic computing}, Routledge, pp. 62–79.
		
		\item[] Feenberg, A. (2002). \textit{Transforming technology: A critical theory revisited}. Oxford University Press.
		
		\item[] Fogg, B. J. (1999). Persuasive technologies. \textit{Communications of the ACM}, 42(5), 27–29.
		
		\item[] Friedman, B., \& Hendry, D. G. (2019). \textit{Value sensitive design: Shaping technology with moral imagination}. MIT Press.
		
		\item[] Friedman, B., Hendry, D. G., Borning, A., et al. (2017). A survey of value sensitive design methods. \textit{Foundations and Trends® in Human–Computer Interaction}, 11(2), 63–125.
		
		\item[] Friedman B, Kahn PH, Borning A (2008). Value sensitive design and information systems. In: \textit{The handbook of information and computer ethics}, pp. 69–101.
		
		\item[] Friedman, B., \& Nissenbaum, H. (1996). Bias in computer systems. \textit{ACM Transactions on Information Systems (TOIS)}, 14(3), 330–347.
		
		\item[] Gandy, O. H. (2010). Engaging rational discrimination: Exploring reasons for placing regulatory constraints on decision support systems. \textit{Ethics and Information Technology}, 12(1), 29–42.
		
		\item[] Giritli Nygren, K. (2009). The rhetoric of e-government management and the reality of e-government work: The Swedish action plan for e-government considered. \textit{International Journal of Public Information Systems}, 2, 135–146.
		
		\item[] Goldman, E. (2008). Search engine bias and the demise of search engine utopianism. In: \textit{Web Search}, Springer, pp. 121–133.
		
		\item[] Grimmelmann, J. (2010). Some skepticism about search neutrality. \textit{The next digital decade: Essays on the future of the Internet}, p. 435.
		
		\item[] Harjumaa, M., \& Oinas-Kukkonen, H. (2007). Persuasion theories and it design. In: \textit{International Conference on Persuasive Technology}, Springer, pp. 311–314.
		
		\item[] Heidegger, M. (1954). The question concerning technology. translated by William Lovitt in \textit{the question concerning technology and other essays}. 1977.
		
		\item[] Hildebrandt, M. (2015). \textit{Smart Technologies and the End (s) of Law: Novel Entanglements of Law and Technology}. Edward Elgar Publishing.
		
		\item[] Ihde, D. (1983). \textit{Existential technics}. SUNY Press.
		
		\item[] Kiran, A. H., \& Verbeek, P. P. (2010). Trusting our selves to technology. \textit{Knowledge, Technology \& Policy}, 23(3–4), 409–427.
		
		\item[] Kitchin, R. (2017). Thinking critically about and researching algorithms. \textit{Information, Communication \& Society}, 20(1), 14–29.
		
		\item[] Latour, B. (1993). \textit{We have never been modern}. Harvard University Press.
		
		\item[] Lazzarato, M., \& Jordan, J. D. (2014). \textit{Signs and machines: Capitalism and the production of subjectivity}. Semiotext (e) Los Angeles.
		
		\item[] Lessig, L. (2006). \textit{Code Version 2.0}. Basic Books.
		
		\item[] Manders-Huits, N. (2011). What values in design? the challenge of incorporating moral values into design. \textit{Science and Engineering Ethics}, 17(2), 271–287.
		
		\item[] Mittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., \& Floridi, L. (2016). The ethics of algorithms: Mapping the debate. \textit{Big Data \& Society}, 3(2), 2053951716679679.
		
		\item[] Noorman, M. (2020). Computing and moral responsibility. In E. N. Zalta (Ed.), \textit{The Stanford Encyclopedia of Philosophy}, Spring 2020th Edition. Metaphysics Research Lab, Stanford University.
		
		\item[] Pariser, E. (2011). \textit{The filter bubble: What the Internet is hiding from you}. Penguin UK.
		
		\item[] Skitka, L. J., Mosier, K., \& Burdick, M. D. (2000). Accountability and automation bias. \textit{International Journal of Human-Computer Studies}, 52(4), 701–717.
		
		\item[] Skitka, L. J., Mosier, K. L., \& Burdick, M. (1999). Does automation bias decision-making? \textit{International Journal of Human-Computer Studies}, 51(5), 991–1006.
		
		\item[] Sparrow, B., Liu, J., \& Wegner, D. M. (2011). Google effects on memory: Cognitive consequences of having information at our fingertips. \textit{Science}, 333(6043), 776–778.
		
		\item[] Sparrow, B. C., Chapman, P., \& Gould, J. (2005). Social cognition in the internet age: Same as it ever was? pp. 273–292.
		
		\item[] Spiekermann, S. (2015). \textit{Ethical IT innovation: A value-based system design approach}. CRC Press.
		
		\item[] Stiegler, B. (1998). \textit{Technics and time: The fault of Epimetheus} (Vol. 1). Stanford University Press.
		
		\item[] Stiegler, B. (2012). Relational ecology and the digital pharmakon. \textit{Culture Machine}, 13.
		
		\item[] Thaler, R. H., \& Sunstein, C. R. (2009). \textit{Nudge: Improving decisions about health, wealth, and happiness}. Penguin.
		
		\item[] van den Berg, B., \& Leenes, R. E. (2013). Abort, retry, fail: Scoping techno-regulation and other techno-effects. In: \textit{Human law and computer law: Comparative perspectives}, Springer, pp. 67–87.
		
		\item[] Van den Hoven, J. (2008). Moral methodology and information technology. \textit{The handbook of information and computer ethics}, p. 49.
		
		\item[] van den Hoven, J., Vermaas, P. E., \& van de Poel, I. (2015). Design for values: An introduction. In: \textit{Handbook of ethics, values, and technological design: Sources, theory, values and application domains} pp. 1–7.
		
		\item[] Van Dijck, J. (2013). \textit{The culture of connectivity: A critical history of social media}. Oxford University Press.
		
		\item[] Veale, M., \& Edwards, L. (2018). Clarity, surprises, and further questions in the article 29 working party draft guidance on automated decision-making and profiling. \textit{Computer Law \& Security Review}, 34(2), 398–404.
		
		\item[] Verbeek, P. P. (2005). \textit{What things do: Philosophical reflections on technology, agency, and design}. Penn State Press.
		
		\item[] Verbeek, P. P. (2011). \textit{Moralizing technology: Understanding and designing the morality of things}. University of Chicago Press.
		
		\item[] Wihlborg, E., Larsson, H., \& Hedström, K. (2016). “The computer says no!”—A case study on automated decision-making in public authorities. In: \textit{2016 49th Hawaii International Conference on System Sciences (HICSS)}, IEEE, pp. 2903–2912.
		
		\item[] Winkler, T., \& Spiekermann, S. (2018). Twenty years of value sensitive design: A review of methodological practices in VSD projects. \textit{Ethics and Information Technology}. pp. 1–5.
		
		\item[] Wittkower, D. (2014). Facebook and dramauthentic identity: A post-goffmanian theory of identity performance on SNS. \textit{First Monday}, 19(4).
		
	\end{itemize}
\end{latin}