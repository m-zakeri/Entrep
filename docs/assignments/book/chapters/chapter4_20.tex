% =================================================================
% فایل chapter4_20.tex (نسخه نهایی و کامل با اصلاح شماره‌گذاری)
% =================================================================

\clearpage % شروع از صفحه جدید

% -----------------------------------------------------------------
% تنظیمات حیاتی شماره‌گذاری (حل مشکل ۱.۲۰ -> ۲۰.۱)
% -----------------------------------------------------------------
\setcounter{section}{20} 
\setcounter{subsection}{0} 
\renewcommand{\thesubsection}{20.\arabic{subsection}} 

% -----------------------------------------------------------------
% تیتر اصلی فصل ۲۰
% -----------------------------------------------------------------
\vspace*{0.5cm}
\begin{flushright}
	{\huge \textbf{۲۰. \hspace{0.2cm} اخلاق داده و علم داده: یک پیوند دشوار؟}}
	\addcontentsline{toc}{section}{۲۰. اخلاق داده و علم داده: یک پیوند دشوار؟}
\end{flushright}

\vspace{1cm}

\begin{center}
	\large \textbf{استر کیمولن و لینت تیلور} \\
	\lr{(Esther Keymolen \& Linnet Taylor)}
\end{center}

\vspace{1cm}

% -----------------------------------------------------------------
% فهرست محتویات فصل
% -----------------------------------------------------------------
\noindent \textbf{\large محتویات فصل}
\begin{description}
	\setlength\itemsep{0em}
	\item[] ۲۰.۱ مقدمه \dotfill \pageref{sec:20-intro}
	\item[] ۲۰.۲ اخلاق داده در محیط آکادمیک \dotfill \pageref{sec:20-academia}
	\begin{itemize}
		\item[] ۲۰.۲.۱ نظریه‌های اخلاقی \dotfill \pageref{subsec:20-moral-theories}
		\item[] ۲۰.۲.۲ پیامدگرایی \dotfill \pageref{subsec:20-consequentialism}
		\item[] ۲۰.۲.۳ اخلاق وظیفه‌گرا \dotfill \pageref{subsec:20-deontological}
		\item[] ۲۰.۲.۴ اخلاق فضیلت \dotfill \pageref{subsec:20-virtue}
		\item[] ۲۰.۲.۵ کانون توجه اخلاق داده آکادمیک \dotfill \pageref{subsec:20-focus}
	\end{itemize}
	\item[] ۲۰.۳ اخلاق داده در حوزه تجاری \dotfill \pageref{sec:20-commercial}
	\begin{itemize}
		\item[] ۲۰.۳.۱ سطح فناورانه \dotfill \pageref{subsec:20-tech-level}
		\item[] ۲۰.۳.۲ سطح فردی \dotfill \pageref{subsec:20-individual-level}
		\item[] ۲۰.۳.۳ سطح سازمانی \dotfill \pageref{subsec:20-org-level}
	\end{itemize}
	\item[] ۲۰.۴ قانون و اخلاق داده \dotfill \pageref{sec:20-law}
	\item[] ۲۰.۵ اخلاق داده و علم داده: آیا این همراهی پایدار است؟ \dotfill \pageref{sec:20-long-run}
	\item[] مراجع \dotfill \pageref{sec:20-references}
\end{description}

\newpage

% =================================================================
% کادر اهداف یادگیری
% =================================================================
\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{\large اهداف یادگیری}
		\vspace{0.2cm}
		در پایان این فصل، خواننده باید بتواند:
		\begin{itemize}
			\item تمایز میان سه نظریه اخلاقی کلیدی (پیامدگرایی، اخلاق وظیفه‌گرا و اخلاق فضیلت) را تشخیص دهد.
			\item مثال‌هایی ارائه دهد که چگونه می‌توان از نظریه‌های اخلاقی برای تحلیل فعالیت‌های علم داده استفاده کرد.
			\item تفاوت‌های اخلاق داده در محیط آکادمیک و محیط تجاری را مقایسه کند.
			\item تعامل میان قانون و اخلاق را در حوزه علم داده تبیین کند.
			\item ارزیابی نقادانه‌ای از کاربردهای مختلف اخلاق داده داشته باشد.
		\end{itemize}
		\vspace{0.1cm}
	\end{minipage}
}
\vspace{0.5cm}

% =================================================================
% ۲۰.۱ مقدمه
% =================================================================
\subsection{مقدمه}
\label{sec:20-intro}

در دو دهه گذشته، شرکت‌های داده‌محور نحوه عملکرد جامعه را دگرگون کرده‌اند. شهروندان به‌طور فزاینده‌ای از پلتفرم‌های رسانه‌های اجتماعی برای ایجاد و حفظ روابط اجتماعی خود استفاده می‌کنند؛ دولت‌ها مداخلات خود را بر اساس ابزارهای الگوریتمی آماده بنا می‌نهند؛ شهرها با جمع‌آوری انواع داده‌های حسگری از ساکنان خود (با همکاری نزدیک با بخش خصوصی) به سمت «هوشمند» شدن حرکت می‌کنند؛ و در حوزه‌های مختلف—از بهداشت تا حقوق—ابتکاراتی برای پیاده‌سازی ابزارهای هوش مصنوعی \lr{(AI)} جهت بهبود فرآیند تصمیم‌گیری وجود دارد. به معنای واقعی کلمه، آسمان دیگر محدودیت نیست، چرا که ایلان ماسک \lr{(Elon Musk)}، کارآفرین حوزه فناوری، در حال کار بر روی امکان انتقال تعداد زیادی از افراد به مریخ به عنوان پاسخی به بحران اقلیمی است \lr{(New York Times, 2019)}.

همراه با این امیدهای بزرگ به آنچه فناوری و به‌ویژه نوآوری‌های داده‌محور برای ما به ارمغان می‌آورند، ما همزمان با حوادث بزرگ و تأثیرگذاری مواجه هستیم که توسط همین فناوری‌ها و نوآوری‌ها ایجاد شده‌اند. از کمبریج آنالیتیکا \lr{(Cambridge Analytica)} که در انتخابات سراسر جهان مداخله می‌کند \lr{(New Statesman, 2018; Observer, 2020)} تا شرکت \lr{Clearview} که رسانه‌های اجتماعی را برای تغذیه برنامه تشخیص چهره خود می‌کاود \lr{(Wired, 2020)}، و از رفتار نامناسب آمازون با کارمندانش \lr{(Guardian, 2020)} تا پیشنهادات جستجوی نژادپرستانه گوگل \lr{(Wired, 2018)}، مشخص می‌شود که شرکت‌های فناوری با پذیرش مسئولیت تأثیرات اجتماعی و سیاسی خود دست‌ و پنجه نرم می‌کنند.

در برابر این پس‌زمینه، تعجبی ندارد که جامعه مدنی در سراسر جهان از شرکت‌های داده‌محور خواسته است تا مسئولیت خود را جدی بگیرند و برای عادلانه‌تر، شفاف‌تر، پاسخگوتر و قابل‌اعتمادتر شدن تلاش کنند (تنها برای نام بردن از چند هدفی که تعیین شده است). اما دقیقاً چه انتظارات اخلاقی می‌توانیم از این شرکت‌ها داشته باشیم و شرکت‌ها برای بهبود خود چه باید بکنند؟ با این آرمان‌ها برای همسوسازی بیشتر شرکت‌های داده‌محور و همچنین محصولات و خدمات آن‌ها با ارزش‌های محوری یک جامعه دموکراتیک و شکوفا، \gls{data-ethics} وارد صحنه می‌شود.

همان‌طور که در این فصل خواهیم دید، \gls{data-ethics} یک مفهوم مبهم \lr{(fuzzy concept)} است که می‌تواند برای افراد مختلف معانی متفاوتی داشته باشد. بنابراین، بخش بزرگی از این فصل به توضیح \gls{data-ethics} از زوایای مختلف اختصاص دارد. ما با مقدمه‌ای کوتاه بر \gls{data-ethics} به عنوان یک رشته دانشگاهی آغاز می‌کنیم و نشان می‌دهیم که چگونه برخی از این دیدگاه‌های آکادمیک در بحث‌های مربوط به \gls{data-science} و هوش مصنوعی نفوذ می‌کنند. سپس، تمرکز خواهیم کرد بر اینکه چگونه \gls{data-ethics} به عنوان یک استراتژی تنظیمی توسط شرکت‌های داده‌محور مطرح شده است تا پاسخی به حوادث و چالش‌های توصیف‌شده در بالا تدوین کنند.

در ادامه، توضیح خواهیم داد که \gls{data-ethics} چگونه با \gls{law} (قانون) ارتباط دارد (که همچنین با سایر فصل‌های این ماژول ارتباط نزدیکی دارد). ما به ریسک «اخلاق‌شویی» \lr{(ethics washing)} خواهیم پرداخت؛ حالتی که در آن اگر \gls{data-ethics} در این بافت کارآفرینی به‌درستی نهادینه نشود، ممکن است به عنوان راه فراری از مقررات قانونی مورد استفاده قرار گیرد. ما این فصل را با تأملی بر رابطه آینده \gls{data-ethics} و \gls{data-science} به پایان می‌بریم و تعدادی سؤال بحث‌برانگیز برای تحریک بحث‌های بیشتر ارائه می‌دهیم.

% =================================================================
% ۲۰.۲ اخلاق داده در محیط آکادمیک
% =================================================================
\raggedbottom
\subsection*{۲۰.۲ \gls{data-ethics} در \gls{academia}}
\addcontentsline{toc}{subsection}{۲۰.۲ \gls{data-ethics} در \gls{academia}}
\label{sec:20-academia}

بیایید ابتدا به بخش «اخلاق» در \gls{data-ethics} نگاه کنیم. اخلاق شاخه‌ای از فلسفه است که حول این پرسش می‌چرخد: «چگونه باید عمل کرد؟» این رشته به روشی سیستماتیک، دلایل و استانداردهای زیربنای اعمال ما را مطالعه می‌کند و بررسی می‌کند که چه چیزی اعمال ما را از نظر اخلاقی درست یا غلط، و خوب یا بد می‌سازد \lr{(Timmons, 2012, p. 4)}. «از نظر اخلاقی» در اینجا یک صفت مهم است، زیرا دامنه نوع اعمالی که به آن‌ها علاقه‌مند هستیم را تعیین می‌کند. یک مثال:

\vspace{0.3cm}
\noindent \textbf{شما باید سوپ خود را با قاشق بخورید.}

این جمله دستوری \lr{(prescriptive)} است. به شما می‌گوید چگونه عمل کنید، احتمالاً بر اساس دلایل کارایی و آداب معاشرت. این جمله هنجاری \lr{(normative)} است، به این معنا که به شما می‌گوید عمل مناسب در یک موقعیت خاص چیست (استفاده از قاشق). با این حال، اگرچه این یک جمله هنجاری است، اما لزوماً یک جمله اخلاقی نیست. اینکه سوپ را با قاشق بخورید یا نه، بر ارزش‌های کلیدی مانند کرامت انسانی، آزادی یا رفاه تأثیر نمی‌گذارد. عموماً، این کار با شانس زندگی خوب شما یا دیگران تداخلی ندارد. خوردن سوپ با قاشق یک وظیفه اخلاقی نیست؛ بلکه بیشتر مسئله داشتن آداب معاشرت خوب است. بنابراین، اینکه آیا باید سوپ را با قاشق بخورید یا خیر، سوالی نیست که اخلاق نگران آن باشد.

\vspace{0.3cm}
\noindent \textbf{شما نباید دروغ بگویید.}

مشابه مورد قبلی، این جمله نیز دستوری است و به شما می‌گوید عمل درست چیست (دروغ نگفتن). با این حال، برخلاف جمله «سوپ با قاشق»، این جمله بدیهتاً با برخی از جنبه‌های اصلی زندگی یک فرد تماس دارد. این جمله فوراً با ارزش‌های کلیدی مانند آزادی، رفاه و کرامت انسانی شما تعامل دارد. اینکه به این جمله پایبند باشید یا خیر، تأثیر قابل‌توجهی بر شانس زندگی خوب یک فرد دارد. بنابراین، این یک جمله هنجاری است که شامل یک «بایدِ اخلاقی» \lr{(moral ought)} می‌شود. نقض این گزاره هنجاری، مسئله رفتار بد (بی‌ادبی) نیست، بلکه رفتاری شدیداً غیراخلاقی یا نامشروع محسوب می‌شود.

% =================================================================
% ۲۰.۲.۱ نظریه‌های اخلاقی
% =================================================================
\subsubsection*{۲۰.۲.۱ \glspl{moral-theory}}
\addcontentsline{toc}{subsubsection}{۲۰.۲.۱ \glspl{moral-theory}}
\label{subsec:20-moral-theories}

ما ممکن است به‌طور شهودی بگوییم که دروغ گفتن غیراخلاقی است، اما آیا می‌توانیم توضیحی منسجم و عقلانی نیز ارائه دهیم که چرا اشتباه است؟ چندین \gls{moral-theory} توسعه یافته‌اند تا توضیح دهند چه چیزی یک عمل خاص را از نظر اخلاقی قابل‌قبول (یا غیرقابل‌قبول) می‌سازد. ما به‌طور بسیار مختصر به سه نظریه اخلاقی تأثیرگذار خواهیم پرداخت: \gls{consequentialism}، \gls{deontological-ethics} و \gls{virtue-ethics}. برای هر نظریه، ما مثالی اضافه کرده‌ایم که نشان می‌دهد چگونه این دیدگاه‌های خاص می‌توانند در درک مسائل \gls{data-science} نقش ایفا کنند.

% =================================================================
% ۲۰.۲.۲ پیامدگرایی
% =================================================================
\subsubsection*{۲۰.۲.۲ \gls{consequentialism}}
\addcontentsline{toc}{subsubsection}{۲۰.۲.۲ \gls{consequentialism}}
\label{subsec:20-consequentialism}

\gls{consequentialism} (پیامدگرایی) بر این باور است که برای قضاوت اخلاقی در مورد اعمال خاص، فرد باید صرفاً بر پیامدهای آن اعمال تمرکز کند. به عبارت دیگر، این نظریه «تجسم شهودِ پایه‌ای است که آنچه بهترین یا درست است، هر آن چیزی است که جهان را در آینده به بهترین شکل درآورد» \lr{(Sinnott-Armstrong, 2019)}. آنچه اهمیت دارد، تأثیر اعمال ماست.

در قرن هجدهم، جرمی بنتام \lr{(Jeremy Bentham)} ([1789] 1996) یک نظریه پیامدگرای مشهور به نام فایده‌گرایی \lr{(utilitarianism)} را توسعه داد. در فایده‌گرایی کلاسیک، یک عمل زمانی از نظر اخلاقی درست تلقی می‌شود که «خیر» را به حداکثر برساند. بنتام ادعا می‌کند که تنها ارزشی که فی‌نفسه خوب است، «لذت» است. بنابراین، این ارزش ذاتی باید راهنمای تمام اعمال ما باشد. بدین ترتیب، از دیدگاه فایده‌گرایی، وظیفه اخلاقی ما به حداکثر رساندن خیر، یا همان به حداکثر رساندن لذت است.

برای تعیین اینکه آیا یک عمل به‌طور مؤثر بیشترین لذت یا خوشبختی را برای بیشترین تعداد افراد به ارمغان می‌آورد یا خیر، بنتام پیشنهاد می‌کند که از یک «ترازنامه اخلاقی» \lr{(moral balance sheet)} استفاده شود؛ نوعی تحلیل هزینه-فایده که سعی دارد محاسبه کند هنگام انجام یک عمل، چقدر خوشبختی/ناراحتی می‌توان انتظار داشت. یک ترازنامه باید امکان مقایسه اعمال ممکنِ مختلف را فراهم کند و تعیین نماید کدام‌یک بیشترین خوشبختی را به ارمغان می‌آورد و بنابراین، از دیدگاه فایده‌گرایی، بهترین عمل است. بنابراین، اگر دروغ گفتن رفاه را به حداکثر برساند—یا به تعبیر بنتام، خوشبختی را—آن‌گاه از نظر اخلاقی کار درستی است.

\vspace{0.5cm}
\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{$\blacktriangleright$ پیامدگرایی در اقدامات \gls{data-science}}
		\vspace{0.2cm}
		
		به منظور مهار پیامدهای ویرانگر همه‌گیری کووید-۱۹ \lr{(Covid-19)}، بسیاری از راه‌حل‌های داده‌محور پیشنهاد و توسعه یافته‌اند؛ مانند اپلیکیشن‌های گوشی هوشمند برای امکان ردیابی تماس، ارائه گواهی مصونیت، یا تنظیم دسترسی به ساختمان‌ها و خدمات خاص. همچنین، کاربردهای هوش مصنوعی مانند تشخیص چهره به عنوان راه‌حلی برای شناسایی افرادی که در مجاورت افراد آلوده قرار گرفته‌اند، مطرح می‌شوند. یک نگرانی تکرار شونده این است که این راه‌حل‌های داده‌محور حریم خصوصی شهروندان را نقض می‌کنند. برای مثال، مردم ممکن است کنترل داده‌های شخصی خود را از دست بدهند و بدون رضایت خود شناسایی و دسته‌بندی شوند.
		
		هوان تون-تات \lr{(Hoan Ton-That)}، هم‌بنیان‌گذار شرکت تشخیص چهره \lr{Clearview AI}، توضیح می‌دهد که این نوع راه‌حل‌ها «کمی از حریم خصوصی ما را می‌گیرند» \lr{(NBC News, 2020)} اما برای حل مشکل بزرگ سلامتی که بر ما تحمیل شده، مورد نیاز هستند. به عبارت دیگر، در اینجا خیر بر شر می‌چربد. بله، ما بخشی از حریم خصوصی خود را از دست می‌دهیم، اما در نهایت وضعیت بهتری خواهیم داشت زیرا در این شرایط سلامتی مهم‌تر است.
		
		این شیوه تفکر ذاتاً پیامدگراست. چه عملی رفاه را به حداکثر می‌رساند: «حفاظت از سلامت» یا «حفاظت از حریم خصوصی»؟ با قاب‌بندی مسئله به این شکل، تمرکز بر پیامدهای عمل قرار می‌گیرد و یک تحلیل هزینه-فایده را تحریک می‌کند. این دیدگاه فرض می‌کند که به طریقی می‌توان کمی‌سازی کرد که چقدر «خیر» از طریق حفاظت از سلامت و چقدر از طریق حفاظت از حریم خصوصی به دست می‌آوریم. اگر حفاظت از سلامت با معرفی این فناوری‌های غیردوستدار حریم خصوصی، رفاه کلی ما را به حداکثر برساند، آنگاه وظیفه اخلاقی ما انجام این کار است، حتی اگر در این فرآیند برخی افراد از نقض حریم خصوصی خود رنج ببرند.
		\vspace{0.1cm}
	\end{minipage}
}
\vspace{0.5cm}

% =================================================================
% ۲۰.۲.۳ اخلاق وظیفه‌گرا
% =================================================================
\subsubsection*{۲۰.۲.۳ \gls{deontological-ethics}}
\addcontentsline{toc}{subsubsection}{۲۰.۲.۳ \gls{deontological-ethics}}
\label{subsec:20-deontological}

در \gls{deontological-ethics}، یک عمل زمانی از نظر اخلاقی درست تلقی می‌شود که شما بر اساس وظیفه اخلاقی خود عمل کنید. یک روایت تأثیرگذار از اخلاق وظیفه‌گرا، اخلاق کانتی است که توسط فیلسوف آلمانی قرن هجدهم، ایمانوئل کانت \lr{(Immanuel Kant)} ([1785] 1997) توسعه یافت. تمرکز در اخلاق کانتی بر پیامد عمل نیست (آن‌طور که در مورد \gls{consequentialism} صدق می‌کند)، بلکه بر دلیلِ درگیر شدن در یک عمل خاص است.

طبق نظر کانت، یک عمل غیراخلاقی، عملی است که مغایر با عقل باشد. یک عمل درستِ اخلاقی، عملی است که با قانون اخلاقی یا آنچه او «امر مطلق» \lr{(categorical imperative)} می‌نامید، مطابقت داشته باشد. دو صورت‌بندی از امر مطلق عبارتند از:
\begin{enumerate}
	\item «تنها بر پایه آن قاعده‌ای \lr{(maxim)} عمل کن که در عین حال بخواهی که آن قاعده به یک قانون جهان‌شمول تبدیل شود.»
	\item «چنان عمل کن که انسانیت را، چه در شخص خود و چه در شخص دیگران، همواره و در عین حال به عنوان یک غایت (هدف) ببینی، نه صرفاً به عنوان یک وسیله (ابزار).»
\end{enumerate}
هنگام در نظر گرفتن یک مسیر عمل، فرد باید بررسی کند که آیا دلایل او برای عملش—یا آنچه کانت «قاعده» \lr{(maxim)} می‌نامد—با امر مطلق مطابقت دارد یا خیر.

اگر صورت‌بندی اول را در نظر بگیریم، روشن می‌شود که دروغ گفتن از نظر اخلاقی غیرقابل‌قبول است. ما نمی‌توانیم دنیایی را تصور کنیم که در آن همه قاعده دروغ گفتن را داشته باشند، زیرا دنیایی برای تصور کردن وجود نخواهد داشت که در آن همه دروغ بگویند و من همچنان قادر باشم بر اساس قاعده خود عمل کنم. اما همچنین، صورت‌بندی دوم نشان می‌دهد چرا از دیدگاه کانتی این کار از نظر اخلاقی اشتباه است. دروغ گفتن به افراد ذاتاً به معنای ناکامی در رفتار با آن‌ها به عنوان یک غایت است—یعنی ناکامی در احترام به ظرفیت عقلانی آن‌ها—و بنابراین ناکامی در احترام به انسانیت آن‌هاست.

\vspace{0.5cm}
\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{$\blacktriangleright$ اخلاق وظیفه‌گرا در اقدامات \gls{data-science}}
		\vspace{0.2cm}
		
		در سال ۲۰۲۰، در یک حکم تاریخی، دادگاه منطقه‌ای در لاهه (هلند) به این قضاوت رسید که استفاده از \lr{SyRI}—یک سیستم پروفایل‌سازی ریسک کلاهبرداری رفاهی که توسط نهادهای دولتی ایجاد شده بود—غیرقانونی است. این سیستم منابع داده‌ای مختلف—از داده‌های درآمد و مالکیت خانه تا داده‌های مصرف آب و انرژی—را برای امتیازدهی به افراد تحلیل می‌کرد. بالاترین امتیاز منجر به برچسب «شایسته تحقیق» می‌شد \lr{(BVV, 2018)}. این تحقیق می‌توانست برای مثال به شکل بازدید از منزل برای بررسی مشروعیت مزایای دریافتی یا یک جلسه رسیدگی به عنوان مقدمه‌ای برای اعمال تحریم‌ها باشد.
		
		دادگاه در حکم خود، به‌ویژه بر عدم شفافیت سیستم مورد استفاده تأکید کرد. شهروندان نمی‌دانند که در حال امتیازدهی شدن هستند، برای عموم روشن نیست که دولت چگونه به نتیجه خاصی رسیده است، و شهروندان نمی‌توانند تا حد معقولی پیگیری کنند که چه اتفاقی با داده‌هایشان می‌افتد.
		
		اهمیتی که دادگاه برای شفافیت قائل است را می‌توان از دیدگاه کانتی توضیح داد. با ارائه نکردن توضیحی به شهروندان در مورد اینکه سیستم چگونه به یک تصمیم خاص می‌رسد، آن‌ها از فرصت تأمل در مورد آن محروم می‌شوند. شهروندان نمی‌توانند تصمیم بگیرند که آیا با نتیجه موافق هستند یا خیر. عملکرد مبهم سیستم همچنین امکان اعتراض به تصمیم را برای آن‌ها دشوار می‌کند، زیرا نمی‌توانند گام‌هایی را که منجر به نتیجه شده دنبال کنند. در مجموع، این سیستم عقلانیت انسان‌ها را ارج نمی‌نهد و با آن‌ها صرفاً به عنوان وسیله‌ای در عملکرد سیستم رفتار می‌کند.
		\vspace{0.1cm}
	\end{minipage}
}
\vspace{0.5cm}

% =================================================================
% ۲۰.۲.۴ اخلاق فضیلت
% =================================================================
\subsubsection*{۲۰.۲.۴ \gls{virtue-ethics}}
\addcontentsline{toc}{subsubsection}{۲۰.۲.۴ \gls{virtue-ethics}}
\label{subsec:20-virtue}

\gls{virtue-ethics} (اخلاق فضیلت)، برخلاف دو نوع قبلی نظریه‌های اخلاقی، بر یک قانون اخلاقی یا بر پیامدهای اعمال تمرکز نمی‌کند، بلکه بر ویژگی‌های شخصیتی یا فضایل یک فرد تمرکز دارد. طرفداران اخلاق فضیلت بر جزئیات یک موقعیت خاص تمرکز می‌کنند و اعمال فرد را با تقاضاهای زمینه خاصی که در آن عمل می‌کند، تنظیم می‌کنند. در این دیدگاه، اخلاق قبل از هر چیز درباره توسعه «خرد عملی» \lr{(practical wisdom)} است: توانایی تعیین آنچه از نظر اخلاقی مورد نیاز است، حتی اگر مربوط به یک موقعیت جدید یا غیرمعمول باشد که قوانین عمومی نمی‌توانند به راحتی اعمال شوند.

اخلاق فضیلت درباره «آدم خوبی بودن» است. این نظریه بر توسعه فضایل لازم برای داشتن یک زندگی خوب و شکوفا متمرکز است. زمانی که چیزی از نظر اخلاقی مهم در خطر است، فرد باید این سوال را بپرسد: «یک فرد بافضیلت در این موقعیت چه می‌کرد؟» و از الگوی او پیروی کند.

فیلسوف یونانی، ارسطو \lr{(Aristotle)} (قرن چهارم پیش از میلاد، ۱۹۸۴)، یکی از پدران بنیان‌گذار اخلاق فضیلت است. او نظریه «حد وسط» \lr{(golden mean)} را توسعه داد. ارسطو معتقد است فضایل مورد نیاز برای یک زندگی شکوفا، در میانه دو حد افراط و تفریط قرار دارند. برای مثال، شجاعت در میانه بزدلی (یک حد افراط) و بی‌پروایی (حد افراط دیگر) قرار دارد. فرار از همه خطرات بزدلانه است، در حالی که ریسک کردن بیش از حد، بی‌پروایی است.

برای تبدیل شدن به یک فرد بافضیلت، صرفاً داشتن این فضایل کافی نیست؛ فرد باید آن‌ها را به عمل درآورد. «الگوهای اخلاقی» \lr{(Moral exemplars)}—افرادی که قبلاً این فضایل را توسعه داده‌اند و بافضیلت عمل می‌کنند—در این زمینه مهم هستند زیرا راه پیش رو را به ما نشان می‌دهند. تمرین نیز جنبه مهمی از اخلاق فضیلت است. ما تلاش می‌کنیم، شکست می‌خوریم و از اشتباهات خود در مسیر بافضیلت‌تر شدن درس می‌گیریم. برای رسیدن به یک ارزیابی و عمل اخلاقی صحیح، شما باید ویژگی‌های شخصیتی لازم را توسعه دهید و از خود بپرسید: «آیا یک فرد بافضیلت دروغ می‌گوید؟»

\vspace{0.5cm}
\noindent
\hspace{0.8cm}
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-1.2cm-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\textbf{$\blacktriangleright$ اخلاق فضیلت در اقدامات \gls{data-science}}
		\vspace{0.2cm}
		
		در سال ۲۰۱۸، کارمندان گوگل علیه مشارکت شرکتشان در «پروژه میوِن» \lr{(Project Maven)} اعتراض کردند؛ پروژه‌ای که بر بهبود تحلیل تصاویر ویدئویی ضبط شده توسط پهپادها با همکاری وزارت دفاع ایالات متحده متمرکز بود \lr{(Hicks, 2018)}. کارمندان نگران بودند که این فناوری برای مقاصد مرگبار استفاده شود، برای مثال با انتخاب اهداف انسانی برای حملات هوایی.
		
		کارمندان گوگل ابتدا نگرانی‌های خود را در داخل شرکت مطرح کردند. وقتی این کار به پاسخ رضایت‌بخشی منجر نشد، اعتراض آن‌ها صریح‌تر شد. حدود ۴۰۰۰ کارمند گوگل طوماری را امضا کردند و خواستار تغییر سیاست گوگل و تعهد به عدم مشارکت در ساخت فناوری‌های جنگی شدند. برخی از آن‌ها در اعتراض به فعالیت‌های گوگل استعفا دادند. اعتراضات کارمندان گوگل در نهایت منجر شد که گوگل این پروژه را پایان دهد.
		
		از دیدگاه \gls{virtue-ethics}، کارمندان معترض گوگل را می‌توان «الگوهای اخلاقی» در نظر گرفت. در این موقعیت بسیار حساس، آن‌ها توانستند تشخیص دهند چه چیزی در خطر است و مسئولیت خود را پذیرفتند. آن‌ها جزئیات موقعیت را در نظر گرفتند و اقدامات خود را بر اساس آن تنظیم کردند. با این کار، آن‌ها ویژگی‌های شخصیتی خاصی را به عمل درآوردند. آن‌ها با گفتن حقیقت به قدرت، «شجاعت»؛ با همکاری برای تنظیم نامه، «روحیه‌ی همکاری»؛ و با قربانیان احتمالی این فناوری، «شفقت» نشان دادند.
		\vspace{0.1cm}
	\end{minipage}
}
\vspace{0.5cm}

\noindent
صرف واقعیت بحث در مورد سه نظریه اخلاقی، نشان‌دهنده این است که همیشه یک اجماع روشن در مورد اینکه چه چیزی یک عمل را از نظر اخلاقی درست یا غلط، و خوب یا بد می‌سازد، وجود ندارد. با این حال، این نباید منجر به نوعی نسبی‌گرایی «هر چه پیش آید خوش آید» \lr{("anything goes" relativism)} شود. برعکس، این امر باید دانشمندان داده را ترغیب کند تا زمان و تلاش زیادی را صرف توضیح دلایل و اصولی کنند که تصمیماتشان را بر آن بنا می‌کنند. آن‌ها باید فعالانه به دنبال دیگران باشند تا تأملات اخلاقی خود را با آن‌ها به اشتراک بگذارند و ببینند آیا استدلال‌های مخالف ارزشمندی وجود دارد که آن‌ها در نظر نگرفته‌اند (همچنین نگاه کنید به \lr{Leonelli, 2016}).

متأسفانه نقد و بررسی انتقادی این نظریه‌های اخلاقی فراتر از دامنه این فصل است، اما امیدواریم شما هنگام مرور این بررسی کوتاه، خودتان به برخی سؤالات انتقادی فکر کرده باشید. برای مثال: چگونه می‌توانیم به‌طور عینی عملیاتی کنیم و بسنجیم که یک عمل چقدر خوشبختی به بار می‌آورد؟ آیا فکر می‌کنم قابل‌قبول است که برخی افراد رنج ببرند اگر اکثریت سود ببرند؟ آیا همیشه ممکن است در نظر بگیریم که آیا دلایل عمل من قابلیت جهان‌شمول شدن دارند؟ اگر باید به عنوان یک فرد بافضیلت عمل کنم، چگونه می‌توانم یک الگوی اخلاقی را در زندگی روزمره شناسایی کنم؟

همان‌طور که در ادامه این فصل خواهیم دید، اخلاق و تأمل اخلاقی همانند پیروی از یک دستورپخت \lr{(recipe)} نیستند. این کار تضمین نمی‌کند که شما در نهایت به یک غذای اخلاقی عالی برسید. این تلاشی بی‌پایان \lr{(open-ended endeavor)} است که نیازمند توجه مداوم است.

% =================================================================
% ۲۰.۲.۵ کانون توجه اخلاق داده آکادمیک
% =================================================================
\subsubsection*{۲۰.۲.۵ کانون توجه \gls{data-ethics} آکادمیک}
\addcontentsline{toc}{subsubsection}{۲۰.۲.۵ کانون توجه \gls{data-ethics} آکادمیک}
\label{subsec:20-focus}

بیایید اکنون به \gls{data-ethics}—یا گاهی اوقات اخلاق هوش مصنوعی (\lr{AI ethics})—به عنوان زیرمجموعه‌ای خاص از اخلاق بپردازیم. در حوزه آکادمیک، \gls{data-ethics} بدین صورت تعریف می‌شود:

\begin{quote}
	«شاخه‌ای جدید از اخلاق که مشکلات اخلاقی مربوط به **داده‌ها** (شامل تولید، ضبط، مدیریت، پردازش، انتشار، اشتراک‌گذاری و استفاده)، **الگوریتم‌ها** (شامل هوش مصنوعی، عامل‌های مصنوعی، یادگیری ماشین و ربات‌ها) و **رویه‌های متناظر** (شامل نوآوری مسئولانه، برنامه‌نویسی، هک کردن و کدهای حرفه‌ای) را مطالعه و ارزیابی می‌کند تا راه‌حل‌های اخلاقی خوب (مانند رفتارهای درست یا ارزش‌های درست) را فرموله و پشتیبانی نماید.» \lr{(Floridi \& Taddeo, 2016, p. 1)}
\end{quote}

در این حوزه نوظهور، دانشگاهیان با پیشینه‌های مختلف—از اخلاق‌شناسان تمام‌عیار گرفته تا مورخان، قوم‌نگاران، پژوهشگران حقوقی، دانشمندان علوم کامپیوتر و ریاضی‌دانان—بر چالش‌های خاصی که توسط اقدامات \gls{data-science} ایجاد شده، تمرکز می‌کنند. نظریه‌های اخلاقی که در بالا به‌طور مختصر بحث کردیم، سوخت برخی از این مطالعات اخلاق داده را تأمین می‌کنند. برای مثال، می‌توان پرسش‌های اخلاقی ناشی از معرفی خودروهای خودران را از طریق لنز یک نظریه اخلاقی یا ترکیبی از آن‌ها تحلیل کرد \lr{(Nyholm, 2018)}. با این حال، خوب است توجه داشته باشیم که گاهی اوقات این نظریه‌های اخلاقی تنها به‌طور ضمنی در پس‌زمینه کار می‌کنند یا به‌سادگی غایب هستند؛ برای مثال، زمانی که پژوهش ماهیت توصیفی‌تری دارد یا یک ارزش خاص—مانند انصاف یا عدالت—یا رویکرد مبتنی بر حقوق بشر را به عنوان نقطه شروع می‌گیرد.

یک مثال از این پیش‌فرضِ پس‌زمینه‌ای نظریه اخلاقی، پروژه «ماشین اخلاقی» \lr{(Moral Machine)} است (فصل ۲۱ را ببینید) که توسط محققان \lr{MIT} ایجاد و بعداً در نشریه \lr{Nature} منتشر شد \lr{(Awad et al., 2018)}. این پروژه شامل جمع‌سپاری \lr{(crowdsourcing)} آنلاین نظرات ۴۰ میلیون نفر درباره تصمیم‌گیری توسط ماشین‌ها در رابطه با سوالات اخلاقی مختلف بود که با مثال خودروی خودران آغاز می‌شد. این پروژه از یک آزمایش فکری مشهور به نام «مسئله تراموا» \lr{(trolley problem)} سرچشمه گرفت که در آن یک ناظر باید انتخاب کند که آیا یک نفر را نجات دهد یا فرد دیگری (یا گروهی) را از برخورد تراموای فراری. این مسئله به‌طور سنتی به عنوان مسئله «انجام دادن در مقابل اجازه دادن به آسیب» تحلیل شده است \lr{(Woollard \& Howard-Snyder, 2002)}، اما پروژه \lr{MIT} با افزودن موضوع خودروهای بدون راننده، بُعد متفاوتی به استفاده از این مسئله بخشید. اکنون، آزمایش به جای اینکه فرد را به عنوان «یک ناظر» قرار دهد و به تفکر انتزاعی وا دارد، از شرکت‌کننده می‌خواهد که آزمایش فکری را به دنیای واقعی ترجمه کند و آن را به‌طور خاص بر توسعه یک کالای مصرفی، یعنی نوع خاصی از خودرو، اعمال نماید.

این قاب‌بندی، مفروضات متعددی را اضافه می‌کند که در نسخه اصلی وجود نداشت: اینکه حضور خودروها در جاده‌ها هم اجتناب‌ناپذیر و هم ضروری است، اینکه مردم باید تعداد مشخصی از مرگ‌ومیرها را به عنوان نتیجه بپذیرند، و اینکه تنها عنصر انتخاب اخلاقیِ موجود، شکل دادن به نحوه وقوع آن مرگ‌ها از طریق تصمیم‌گیری میان خطای خودکار و خطای راننده است.

یک رویکرد جایگزین ممکن است دامنه پرسش اخلاقی را گسترش دهد تا بپرسد چه ارزش‌هایی باید در شکل‌دهی به سیاست کلی حمل‌ونقل مرکزی باشند. برای مثال، ما ممکن است بپرسیم که آیا سیاست حمل‌ونقل باید بر اشکال عمومی حمل‌ونقل تمرکز کند یا رانندگی با خودرو را به عنوان شیوه اصلی سفر تشویق نماید؛ آیا تمام مرگ‌ومیرهای مرتبط با صنعت خودرو (شامل آلودگی و سهم بخش انرژی در تغییرات اقلیمی)، و نه فقط تصادفات رانندگی، باید هنگام تصمیم‌گیری در مورد نحوه سفر مردم در نظر گرفته شوند؛ یا اینکه آیا یک چشم‌انداز فردی به جای جمعی، موجه‌ترین رویکرد برای این مسئله است. محققان با قرار دادن تنها یک سطح از مسئله به عنوان دوراهی اخلاقی، به‌طور ضمنی سایر مسیرهای ممکن برای تحقیق را مسدود می‌کنند.

در تعریف \gls{data-ethics} از فلوریدی و تادئو \lr{(Floridi \& Taddeo)}، تأکید از یک سو بر جنبه‌های فناورانه (پردازش داده و الگوریتم‌ها) و از سوی دیگر بر رویه‌هایی است که این جنبه‌های فناورانه در آن تعبیه شده‌اند. در مورد اول، اخلاق داده ممکن است به چالش‌های مربوط به استفاده از داده (مانند جمع‌آوری، تحلیل و انتشار داده که می‌تواند حریم خصوصی افراد را نقض کند) یا استفاده از مدل‌ها (مانند مدلی که ممکن است افراد را به‌اشتباه طبقه‌بندی کرده و باعث آسیب شود) بپردازد \lr{(Saltz \& Dewar, 2019, p. 206)}.

در مورد دوم، سوالات اخلاقی مربوط به رویه‌ها می‌تواند در هر دو سطح فردی و سازمانی مطرح شود. در سطح فردی، پرسش‌هایی مانند «یک کارمند حوزه فناوری یا یک دانشمند داده چه فضایلی را باید پرورش دهد؟» \lr{(Vallor, 2016)} یا «چگونه می‌توان یک دانشمند داده پاسخگو بود؟» ممکن است ایجاد شود. در سطح سازمانی، سوالات مربوط به چگونگی ساختار یک شرکت می‌تواند مرتبط باشد، برای مثال: «چه ارزش‌هایی در کد رفتاری یک شرکت فناوری حضور دارند؟» \lr{(Hagendorff, 2020)} یا «آیا مدل کسب‌وکاره یک شرکت داده‌محور با ارزش‌های کلیدی مانند کرامت انسانی و آزادی بیان شهروندان همسو است؟».

این تدقیق بیشتر در تعریف ارائه‌شده توسط فلوریدی و تادئو بدین معناست که روی‌هم‌رفته، \gls{data-ethics} بر چالش‌های اخلاقی تمرکز دارد که در سه سطح متفاوت اما مرتبط پدید می‌آیند: سطح فناورانه، فردی و سازمانی.

% =================================================================
% ۲۰.۳ اخلاق داده در حوزه تجاری
% =================================================================
\raggedbottom
\subsection*{۲۰.۳ \gls{data-ethics} در \gls{commercial-domain}}
\addcontentsline{toc}{subsection}{۲۰.۳ \gls{data-ethics} در \gls{commercial-domain}}
\label{sec:20-commercial}

\gls{data-ethics} تنها محدود به \gls{academia} (محیط آکادمیک) نیست؛ بلکه در \gls{commercial-domain} (حوزه تجاری) نیز شتاب گرفته است. تحت فشار تمامی حوادثی که آشکار می‌شوند—از نشت داده‌ها و اخبار جعلی گرفته تا دستکاری مردم در فضای آنلاین—و واکنش‌های عمومی محبوبی که این حوادث ایجاد کرده‌اند، شرکت‌های فناوری و همچنین افراد (شامل دانشمندان داده) که در صنعت فناوری کار می‌کنند، به دنبال راه‌هایی برای بهبود عملیات خود هستند، به امید آنکه با این حوادث مقابله کنند.

این یک باور دیرینه است که اگر اعتماد به یک شرکت یا سرویس کاهش یابد، مردم سعی می‌کنند از استفاده از آن اجتناب کنند. این امر هم برای کسب‌وکارها و هم برای کل جامعه که به‌طور فزاینده‌ای به زیرساخت‌های داده‌محور وابسته شده است، زیان‌بار خواهد بود. با این حال، اعتماد بدون «قابلیت اعتماد» \lr{(trustworthiness)}، پوسته‌ای توخالی است. متقاعد کردن مشتریان برای اعتماد به محصول یا سرویس شما با سرمایه‌گذاری روی رابط‌های کاربری پرزرق‌وبرق و محصولات با کاربری آسان، کار ساده‌ای است. اما اگر در پشت این رابط کاربری، طرف‌های تجاری سعی در دستکاری مردم داشته باشند و داده‌ها نشت کنند یا فروخته شوند، تنها مسئله زمان است که سوءرفتار بعدی علنی شود و اعتماد فروبپاشد \lr{(Keymolen, 2016, 2017)}.

اگر شرکت‌های داده‌محور و \gls{data-science} به عنوان یک حرفه بخواهند در بلندمدت باقی بمانند، نیاز دارند که (بیشتر) قابل‌اعتماد شوند. \gls{data-ethics} به عنوان یکی از ابزارهای مهم برای قابل‌اعتمادتر شدن و برای ایجاد، بازیابی و حفظ اعتماد مصرف‌کننده در نظر گرفته شده است \lr{(Hasselbalch \& Tranberg, 2016)}.

% =================================================================
% ۲۰.۳.۱ سطح فناورانه
% =================================================================
\subsubsection*{۲۰.۳.۱ \gls{tech-level}}
\addcontentsline{toc}{subsubsection}{۲۰.۳.۱ \gls{tech-level}}
\label{subsec:20-tech-level}

اخلاق داده شرکتی اشکال گوناگونی به خود می‌گیرد. مشابه حوزه آکادمیک، در حوزه تجاری نیز ابتکارات \gls{data-ethics} سطوح فناورانه، فردی و سازمانی را در بر می‌گیرند.

در سطح فناورانه، هم شرکت‌های چندملیتی و هم استارتاپ‌ها اصول طراحی اخلاقی را تدوین کرده‌اند تا اطمینان حاصل کنند که محصولات و خدماتشان بر پایه ارزش‌های کلیدی مانند شفافیت، پاسخگویی، انصاف و عدم تبعیض بنا شده است. در نگاه اول، این‌ها ارزش‌هایی هستند که همه ما به‌راحتی می‌توانیم بر سر آن‌ها توافق کنیم (به هر حال، چه کسی می‌تواند مخالف انصاف یا عدم تبعیض باشد؟). با این حال، زمانی که فرد تلاش می‌کند تعریفی از این ارزش‌ها ارائه دهد، چه رسد به اینکه بخواهد آن‌ها را در یک محصول یا خدمت پیاده‌سازی کند، وضعیت به‌سرعت پیچیده می‌شود \lr{(Mittelstadt, 2019)}.

برای مثال، بحث‌های زیادی در مورد نحوه عملیاتی کردن «انصاف» در هوش مصنوعی وجود دارد \lr{(Suresh \& Guttag, 2019)}. آیا انصاف باید در سطح گروهی (برابری گروهی) تعریف شود یا در سطح فردی (برابری فردی) \lr{(Chouldechova, 2017)}؟ آیا انصاف عمدتاً باید درباره نتایج منصفانه باشد یا رویه‌های منصفانه \lr{(Grgić-Hlača, 2018)}؟ آیا ممکن است هر دو را داشت؟ همسو با تنوعی که پیش‌تر در مرور کوتاه نظریه‌های اخلاقی با آن مواجه شدیم، در اینجا نیز، در اخلاق داده تجاریِ عمل‌گرا و واقع‌بینانه، می‌بینیم که پاسخ روشنی به این پرسش که معنا و محتوای این ارزش‌ها چه باید باشد، وجود ندارد \lr{(Jobin et al., 2019)}. زمانی که فلسفه سیاسیِ انصاف با رویکردهای علوم کامپیوتر به انصاف تماس پیدا می‌کند، به نظر می‌رسد یک تضاد حل‌نشدی رخ می‌دهد، زیرا «اغلب، یک رویکرد متناسب با زمینه به انصاف که واقعاً جوهره نکات فلسفی مربوطه را تسخیر کند، ممکن است منوط به عواملی باشد که معمولاً در داده‌های موجود یافت نمی‌شوند» \lr{(Binns, 2018, p. 9)}.

در مسئله تعریف ارزش‌ها و تعبیه آن‌ها در طراحی و استفاده فناورانه، اخلاق آکادمیک ممکن است به کمک بیاید، چرا که در اینجا رویکردهای مختلفی برای تعریف و ترجمه ارزش‌ها در فناوری‌ها به‌شیوه‌ای سیستماتیک توسعه یافته‌اند. این رویکردها می‌توانند برای شرکت‌هایی که با عملیاتی کردن اصول طراحی خود دست‌وپنج نرم می‌کنند، مفید باشند. یکی از این رویکردها، «طراحی حساس به ارزش» \lr{(Value Sensitive Design)} \lr{(Simon, 2017; Chen \& Zhu, 2019)} است که در فصل ۲۱ به آن پرداخته خواهد شد، اما روش‌های دیگری نیز توسعه یافته‌اند، مانند رویکرد «ارزش‌هایی که اهمیت دارند» \lr{(VtM)} \lr{(Smits et al., 2019)}.

آنچه اغلب در این رویکردها مورد تأکید قرار می‌گیرد این است که چون ارزش‌ها مفاهیم صریح و قطعی نیستند و برای ذینفعان مختلف معانی متفاوتی دارند، دریافت ورودی از این ذینفعان برای درک این تفاسیر متفاوت از اهمیت بالایی برخوردار است. در واقع، از همان ابتدا، هنگام تعریف مسئله‌ای که فعالیت‌های \gls{data-science} را هدایت می‌کند، تعامل با جوامعی که تحت تأثیر ابزار یا خدمتی که شما توسعه می‌دهید قرار می‌گیرند، حیاتی است. صورت‌بندی مسئله توسط آن‌ها باید پیشرو باشد؛ فناوری باید پیروی کند.

همان‌طور که در فصل ۲۱ به‌طور مفصل‌تر توضیح داده خواهد شد، فناوری یک ابزار خنثی نیست. فناوری می‌تواند روابط قدرت موجود را تحکیم کند یا درهم بشکند، می‌تواند عاملیت کاربران نهایی را تقویت کند یا اقدامات آن‌ها را خنثی سازد، و می‌تواند دسترسی به خدمات را فراهم کند یا افراد را طرد نماید. شما نمی‌توانید تمام این پیامدهای ممکن را به‌تنهایی پیش‌بینی کنید. حتی زمانی که ذینفعان را شامل می‌کنید، همیشه پیامدهایی وجود خواهد داشت که پیش‌بینی نکرده بودید. بنابراین، مشورت با خبرگانِ حوزه یا بخشی که در آن کار می‌کنید نیز مهم است. چنین مشورتی فراتر از یک مرور ادبیات صرف است. توصیه می‌شود با خبرگانی تعامل کنید که دانش و تجربه عملی در حوزه کاربردی شما دارند. همان‌طور که مشخص است، زندگی واقعی همیشه بسیار آشفته‌تر و پیچیده‌تر از آن چیزی است که داده‌ها می‌توانند به شما بگویند. این رویکرد تعاملی شانس بهتری به شما می‌دهد تا درک غنی‌تری از زمینه‌ای که در آن کار می‌کنید توسعه دهید و معنای ارزش‌هایی را که می‌خواهید محصول یا خدمتتان پشتیبانی کند، دریابید.

تلاش برای توسعه روش‌هایی جهت بنا نهادن توسعه فناوری بر پایه ارزش‌ها، تنها محدود به حوزه آکادمیک نیست. شرکت‌ها، سازمان‌های عمومی (یا در همکاری‌های عمومی-خصوصی) و همچنین بازیگران دولتی نیز در حال ارائه استراتژی‌هایی برای پیاده‌سازی پربار ارزش‌ها هستند. این‌ها شامل دستورالعمل‌ها و چک‌لیست‌ها (مثلاً \lr{RSS and IFoA, 2019})، پرسشنامه‌ها، وب‌سایت‌های تعاملی، مطالعات موردی، چارچوب‌ها (مثلاً \lr{PartnershipOnAI, 2019})، بوم‌های ارزش، ارزیابی‌های تأثیر و بسیاری موارد دیگر است. انگیزه‌های متفاوتی برای توسعه این اسناد اخلاق علم داده و هوش مصنوعی قابل تشخیص است: از انگیزه مسئولیت اجتماعی و استفاده از آن به عنوان ابزاری برای تغییر گرفته تا دیدن آن به عنوان یک مزیت رقابتی برای شناخته شدن به عنوان «اخلاقی» \lr{(Hasselbalch \& Tranberg, 2016; Schiff et al., 2020)}.

در حالی که از یک سو امیدوارکننده است که استراتژی‌های بسیار زیادی در حال توسعه هستند، کثرت فعلی استراتژی‌ها تصمیم‌گیری در مورد اینکه کدام‌یک بهترین تناسب را دارد دشوار می‌سازد. اگرچه عواملی وجود دارند که می‌توانند نشان‌دهنده موفقیت این اسناد اخلاق هوش مصنوعی باشند—مانند تعامل با قانون، اختصاصی بودن، گستره دسترسی، قابلیت اجرا، و تکرار و پیگیری \lr{(Schiff et al., 2020, p. 156–157)}—هنوز هیچ تحقیق تطبیقی گسترده‌ای انجام نشده است تا اثربخشی هر یک از این رویکردها را تعیین کند.

% =================================================================
% ۲۰.۳.۲ سطح فردی
% =================================================================
\subsubsection*{۲۰.۳.۲ \gls{individual-level}}
\addcontentsline{toc}{subsubsection}{۲۰.۳.۲ \gls{individual-level}}
\label{subsec:20-individual-level}

تحقیقات تجربی نشان می‌دهد که متخصصان هوش مصنوعی (مانند دانشمندان داده) خود را تا حدی از نظر اخلاقی مسئول تأثیرات اجتماعی برنامه‌های خود می‌دانند. با این حال، آن‌ها همچنین ابراز می‌کنند که عاملیت آن‌ها تا حد زیادی توسط نیروهای قدرتمند شرکتی و دولتی محدود شده است \lr{(Orr \& Davis, 2020)}. در واقع، ما شاهد علاقه فزاینده‌ای به استانداردهای اخلاقی و حرفه‌ای برای هدایت و راهبری اقدامات دانشمندان داده هستیم. کدهای رفتاری \lr{(Codes of conduct)}—اسنادی که در آن‌ها سازمان‌ها (مانند شرکت‌ها یا انجمن‌های حرفه‌ای) دستورالعمل‌هایی برای رفتار حرفه‌ای تعیین می‌کنند—در حال توسعه هستند تا متخصصان این حوزه را راهنمایی کنند، آگاهی اخلاقی را افزایش دهند و بحث‌های اخلاقی را در میان همکاران و در درون شرکت تحریک نمایند \lr{(Van de Poel \& Royakkers, 2011, p. 32–42)}.

انواع مختلفی از کدهای رفتاری وجود دارد. **کدهای آرمانی** \lr{(Aspirational codes)} خطاب به دنیای بیرون هستند و بیانگر آن چیزی هستند که یک شرکت پایبند آن است. برای مثال، مایکروسافت \lr{(Microsoft)} (2020) اصول هوش مصنوعی مسئولانه‌ای را توسعه داده است که فعالیت‌های تجاری آن‌ها را هدایت می‌کند. **کدهای مشورتی** \lr{(Advisory codes)} بر متخصصان متمرکز هستند و هدفشان کمک به آن‌ها در تصمیم‌گیری‌های اخلاقی در کارشان است. برای مثال، یک کد حرفه‌ای برای دانشمندان داده توسعه یافته است تا آن‌ها را در کارشان راهنمایی کند \lr{(Oxford-Munich, 2020)}. همچنین **کدهای انضباطی** \lr{(Disciplinary codes)} وجود دارند. این کدها قوانین پایه‌ای را تعیین می‌کنند تا اطمینان حاصل شود که اقدامات کارمندان با استانداردهای خاصی مطابقت دارد و عمدتاً بر عملکرد داخلی شرکت متمرکز هستند \lr{(Van de Poel \& Royakkers, 2011, p. 32–42)}.

مهم است توجه داشته باشیم که به‌طور کلی، کدهای رفتاری شکلی از «خودتنظیمی» \lr{(self-regulation)} هستند. آن‌ها عموماً توسط قانون الزامی یا اجرا نمی‌شوند. آن‌ها وضعیت قانونی روشنی ندارند. این بدان معناست که مشتریان نمی‌توانند زمانی که معتقدند یک شرکت یا کارمند شرکت کد رفتاری خود را نقض کرده است، به دادگاه مراجعه کنند. علاوه بر این، شرکت‌ها هیچ مسئولیت قانونی برای در دسترس قرار دادن کدهای رفتاری داخلی خود برای کاربران ندارند، که این امر تأکید می‌کند چنین کدهایی برای عمل کردن به عنوان مقررات رسمی طراحی نشده‌اند.

در کنار این کدهای رفتاری، تحول مهم دیگری نیز در سطح فردی اخلاق داده در حال رخ دادن است که می‌توان آن را «اخلاق داده از درون» \lr{(data ethics from within)} نامید. این به کارگران حوزه فناوری اشاره دارد که با اعتراض علیه شرکت‌های خود زمانی که معتقدند به شیوه‌ای غیراخلاقی عمل می‌کنند، حقیقت را به قدرت می‌گویند. پروژه میوِن \lr{(Maven project)} که در بالا در بخش ۲۰.۲.۴ به‌طور مختصر بحث شد، نمونه‌ای از چنین «اخلاق داده از درون» است. جنبش‌های کارگران منتقدِ فناوری نور را می‌بینند و خواستار دانستن این هستند که فناوری‌ای که روی آن کار می‌کنند در واقع قرار است چه هدفی را دنبال کند. آن‌ها طومار امضا می‌کنند، به مدیران اجرایی شرکت‌های خود اعتراض می‌کنند و گاهی حتی شغل خود را ترک می‌کنند تا بر شرکت‌ها و جامعه برای مداخله فشار بیاورند. به‌ویژه در بازار کاری که تقاضای بالایی برای دانشمندان داده، دانشمندان علوم کامپیوتر و سایر متخصصان فنی کلیدی وجود دارد، نفوذ و قدرت این کارمندانِ فردیِ حوزه فناوری نباید دست‌کم گرفته شود، قطعاً نه اگر آن‌ها راهی برای سازماندهی خود و بیان جمعی نگرانی‌ها و اعتراض به اقدامات مشکوک شرکتی پیدا کنند.

% =================================================================
% ۲۰.۳.۳ سطح سازمانی
% =================================================================
\subsubsection*{۲۰.۳.۳ \gls{org-level}}
\addcontentsline{toc}{subsubsection}{۲۰.۳.۳ \gls{org-level}}
\label{subsec:20-org-level}

با تحریک روند \gls{data-ethics} تجاری، شرکت‌ها همچنین در حال سرمایه‌گذاری در انواع نوآوری‌های سازمانی هستند. آن‌ها کمیته‌های اخلاق \lr{(ethics boards)} را برای بررسی شکایات از درون و بیرون شرکت راه‌اندازی می‌کنند. اخلاق‌شناسان استخدام می‌شوند تا تیم‌های طراحی را غنی کنند، و جوامع اخلاقی نصب می‌شوند تا فعالیت‌های شرکت‌ها را پایش کرده و در مورد مسائل مرتبط با اخلاق مشاوره دهند.

در یک وضعیت ایده‌آل، سطوح فناورانه، فردی و سازمانیِ اخلاق داده تجاری در هم قفل می‌شوند. \gls{data-ethics} یک «فرآیند مشارکتی» \lr{(collaborative process)} است و همواره «در حال تغییر» \lr{(in flux)} می‌باشد \lr{(Orr \& Davis, 2020, p. 13)}. کدهای رفتاری و اصول طراحی به گونه‌ای عملیاتی می‌شوند که بتوانند در واقعیت، به شیوه‌ای معنادار، اقدامات کارمندان را هدایت کنند. کارمندان منتقد به عنوان دارایی شرکت شناخته می‌شوند و ساختار شرکت به گونه‌ای است که مشارکت‌های آن‌ها را در فرآیند تصمیم‌گیری شامل شود. به‌طور ایده‌آل، شرکت‌ها طوری تنظیم می‌شوند که هم در برابر کارمندان خود و هم در برابر دنیای بیرون، و به‌طور خاص در برابر جوامعی که محصولات و خدماتشان بر آن‌ها تأثیر می‌گذارد، پاسخگو باشند.

با این حال، همان‌طور که قبلاً اشاره کردیم، تمام این ابتکارات \gls{data-ethics} داوطلبانه هستند. هر شرکتی می‌تواند لیست اصول طراحی اخلاقی خود را در وب‌سایتش منتشر کند، دانشمندان داده بافضیلت را با بیانیه‌های اخلاقی پرشور فریب دهد تا به شرکت بپیوندند، و خود را به دنیای بیرون به عنوان شرکتی اخلاقی و پایدار که منافع کاربران نهایی را در قلب خود دارد معرفی کند، در حالی که در واقعیت ممکن است هیچ اهمیتی به این موضوع ندهند.

بنابراین حیاتی است که تمام این ابتکارات با مکانیزم‌های اجرایی سازمانی در قالب پاسخگویی ساختاری در درون و بیرون شرکت همراه باشند، مانند الزامات گزارش‌دهی و حسابرسیِ آن گزارش‌ها؛ در غیر این صورت، آن‌ها صرفاً در حد شعار و ظاهرسازی \lr{(paying lip service)} برای \gls{data-ethics} باقی می‌مانند. تحقیقات نشان می‌دهد که کار بیشتری باید در این زمینه انجام شود \lr{(Hagendorff, 2020)}. برای مثال، از بیش از ۱۶۰ دستورالعمل اخلاق هوش مصنوعی که جمع‌آوری شد، تنها ۱۰ مورد دارای مکانیزم‌های اجرایی مناسب بودند. علاوه بر این، این کدهای رفتاری و اصول طراحی اخلاقی نسبتاً مبهم و انتزاعی باقی مانده‌اند که پیاده‌سازی واقعی آن‌ها را دشوار می‌سازد. این مسئله این پرسش را مطرح می‌کند: دستورالعمل‌هایی که «نه می‌توانند اعمال شوند و نه اجرا»، آیا «مضرتر از نداشتن هیچ دستورالعمل اخلاقی نیستند»؟

% =================================================================
% ۲۰.۴ قانون و اخلاق داده
% =================================================================
\raggedbottom
\subsection*{۲۰.۴ \gls{law} و \gls{data-ethics}}
\addcontentsline{toc}{subsection}{۲۰.۴ \gls{law} و \gls{data-ethics}}
\label{sec:20-law}

در حوزه تجاری، زمانی که \gls{data-ethics} برای اجتناب از مقررات قانونی استفاده شود، تبدیل به «اخلاق‌شویی» \lr{(ethics washing)} می‌شود \lr{(Wagner, 2018)}. اخلاق‌شویی فرآیندی است که در آن یک شرکت رفتاری اخلاقی از خود نشان می‌دهد تا انتقادات نسبت به رویه‌های زیان‌بار را منحرف کند و بدین ترتیب شهرت خود را «تطهیر» نماید، بدون آنکه مدل کسب‌وکار خود را تغییر دهد. یک مثال از این مورد، شرکت تحلیل داده و نظارت آمریکایی «پالانتیر» \lr{(Palantir)} است که کنفرانس‌های حقوق حریم خصوصی را حمایت مالی می‌کرد، در حالی که همزمان سیستم‌های نظارتی مورد استفاده برای جداسازی خانواده‌های مهاجر در ایالات متحده را توسعه می‌داد \lr{(Guardian, 2019)}.

در این حالت، \gls{data-ethics} به عنوان یک استراتژی خودتنظیمی، نه برای بهبود پاسخگویی، بلکه برای اجتناب از اقدامات تنظیمی سخت‌گیرانه‌ترِ بالا-به-پایین به کار گرفته می‌شود. یک استراتژی اصیل \gls{data-ethics} تنها زمانی می‌تواند توسعه یابد که جایگاه خود را در رابطه با \gls{law} بشناسد. اخلاق، به گفته هیلدبرانت \lr{(Hildebrandt, 2020, p. 297)}:

\begin{quote}
	«هم بیش از قانون است و هم کمتر از آن: بیش از آن است زیرا بسیاری از دغدغه‌های اخلاقی توسط قانون پوشش داده نمی‌شوند؛ و کمتر از آن است زیرا خروجی ملاحظات اخلاقی لزوماً به هنجارهای قانونی تبدیل نمی‌شوند و بنابراین از طریق قانون قابل اجرا نیستند.»
\end{quote}

بنابراین، زمانی که این را بر \gls{data-ethics} تجاری اعمال می‌کنیم، درمی‌یابیم که \gls{data-ethics} می‌تواند بیش از قانون باشد، زیرا می‌تواند استانداردهایی را تعیین کند که توسط قانون الزامی نشده‌اند و در موقعیت‌هایی که مستقیماً توسط قانون پوشش داده نمی‌شوند، راهنمایی ارائه دهد. برای مثال، دانشمندان داده می‌توانند انتخاب کنند که با داده‌های ناشناس به عنوان داده‌های شخصی رفتار کنند و به مقررات عمومی حفاظت از داده‌ها (\lr{GDPR}، که در فصل‌های ۱۷ و ۲۱ نیز بحث شده) پایبند باشند، زیرا آن‌ها تقویت حریم خصوصی را مهم می‌دانند، نه به این دلیل که قانوناً ملزم به آن هستند. در آن صورت، آن‌ها فراتر از انتظارات قانون از خود عمل می‌کنند و وارد قلمرو \gls{data-ethics} می‌شوند.

قانون یک «فضای عمل» \lr{(action space)} فراهم می‌کند که شرکت‌ها می‌توانند در آن رویه‌های اخلاق داده را توسعه دهند؛ با این حال، این رویه‌های اخلاق داده نمی‌توانند و نباید جایگزین قانون شوند، زیرا مهم‌تر از همه، قانون نوعی «فصل‌الخطاب» یا قطعیت \lr{(closure)} فراهم می‌کند که اخلاق نمی‌تواند \lr{(Hildebrandt, 2020)}. در یک جامعه دموکراتیکِ با عملکرد صحیح، شفاف بودن و قابل پیش‌بینی بودن اینکه چه نوع رفتارهایی قابل‌قبول هستند و کدام‌یک نیستند، از اهمیت بالایی برخوردار است \lr{(Tamanaha, 2004, 2007)}.

برای مثال، این ایده که «شما نباید به کسی دروغ بگویید» برای یک جامعه دموکراتیکِ شکوفا آن‌قدر اهمیت دارد که از قلمرو اخلاقی به یک هنجار قانونی تبدیل شده است. مثلاً هنگام شهادت در دادگاه، شما باید قول بدهید که حقیقت را بگویید و ارتکاب کلاهبرداری (که خود نوعی دروغگویی است) خلاف قانون و بنابراین قابل مجازات است.

آنچه یک هنجار قانونی را از یک هنجار اخلاقی متمایز می‌کند، این است که قانون هم «قابل پیش‌بینی» و هم «قابل اجرا» \lr{(enforceable)} است. به عبارت دیگر، شما از پیش می‌دانید چه عملی مناسب تلقی می‌شود و اگر به قانون پایبند نباشید چه انتظاری باید داشته باشید. این نوعی از وضوح و قدرت است که اخلاق نمی‌تواند فراهم کند. ما پیش‌تر در مقدمه اخلاق آکادمیک دیدیم که نظریه‌های اخلاقی متفاوتی وجود دارند که منطق‌های متفاوتی برای آنچه «عمل خوب» محسوب می‌شود، ارائه می‌دهند. این امر اخلاق را در ماهیت خود باز-پایان \lr{(open-ended)} می‌سازد و امکان پرسشگری و تأمل مداوم در تصمیمات و اعمالمان را فراهم می‌کند.

در ایده‌آل‌ترین وضعیت، رویه‌های \gls{data-ethics} اعمال ما را در درونِ فضای عملِ فراهم‌شده توسط قانون آگاه می‌سازند و ما را تشویق می‌کنند تا از «ذهنیت تیک زدن» \lr{(checkbox mentality)} فراتر رویم تا رویه‌های علم داده‌ای را توسعه دهیم که در آن مسئولیت و پاسخگویی نهادینه شده باشد. در این شرایط ایده‌آل، \gls{data-ethics} دانشمندان داده را قادر می‌سازد تا شایستگی‌های اخلاقی خود را توسعه و پرورش دهند، و شرکت‌ها را تسهیل می‌کند تا رویکردهای اخلاق داده‌ی متناسبی را توسعه دهند که با پروفایل و سازمان شرکتشان همخوانی داشته باشد.

با این حال، در چارچوب زمانی فعلی ما، به نظر می‌رسد فناوری‌های به‌سرعت در حال تحول، توانایی قانون را برای فراهم کردن و اجرای این فضای عملِ ضروری به چالش می‌کشند. برای مثال، پیشرفت‌ها در هوش مصنوعی ممکن است منجر به سوالات پیچیده‌ای در مورد توضیح‌پذیری، انصاف و پاسخگویی شود که ما را ملزم به بازاندیشی و تفسیر مجدد هنجارهای قانونی موجود می‌کند. این فرآیندهای دموکراتیک، با این حال، زمان‌بر هستند.

در نگاه اول، ممکن است جذاب به نظر برسد که اجازه دهیم \gls{data-ethics}، که بسیار چابک‌تر و منعطف‌تر از قانون است، به سرعت آن خلأ را پر کند. در این صورت، \gls{data-ethics} دیگر راهی برای توسعه رویه‌های علم داده در درون این فضای عمل نیست، بلکه تبدیل به فراهم‌کننده اصلی آن فضای عمل می‌شود.

اما از آنجا که \gls{data-ethics} فاقد قدرت اجراست و انعطاف‌پذیری آن با پیش‌بینی‌پذیری‌ای که ما از یک فضای عمل دموکراتیک انتظار داریم همخوانی ندارد—مردم تصمیم می‌گیرند چه مسیر اخلاقی را از طریق فرآیند تأمل دنبال کنند، نه با پیروی از قوانین وضع‌شده در قانون، به‌طوری که ممکن است بسته به زمینه، پاسخ «درست» متفاوتی برای یک سوال داده‌شده وجود داشته باشد—اخلاق هرگز نمی‌تواند قطعیتِ لازم را فراهم کند. در نتیجه، یک فضای عمل که بر پایه \gls{data-ethics} تجاری بنا شده باشد، منجر به یک «چهل‌تکه‌ی پراکنده» \lr{(scattered patchwork)} از فضاهای عمل متفاوت و سیال خواهد شد که در آن شرکت‌ها می‌توانند قوانین بازی خود را تعیین کنند و سایر بازیگران (هم شهروندان و هم دولت‌ها) فقط باید با آن همراهی کنند. این منجر به عدم تعادل قدرتی می‌شود که در واقع به جای محافظت از ارزش‌های کلیدی مانند برابری، انصاف و عدالت، آن‌ها را تضعیف خواهد کرد.

این سوءاستفاده شدید و حتی سوءرفتار با \gls{data-ethics}، حوزه سیاست‌گذاری را به این پرسش واداشته است که آیا \gls{data-ethics} به عنوان یک درمان، در واقع بدتر از خودِ بیماری نیست؟ این امر حتی منجر به آنچه بیِتی \lr{(Bietti, 2020)} به عنوان «اخلاق‌کوبی» \lr{(ethics bashing)} اشاره می‌کند، شده است: حمله به کل حوزه اخلاق به دلیل این موارد سوءاستفاده.

اخلاق می‌تواند فرآیندهای دموکراتیکی را که منجر به یک فضای عمل به‌روزرسانی‌شده برای رویه‌های علم داده می‌شود، آگاه سازد، اگر و تنها اگر اخلاق در معنای وسیع کلمه درک شود و نه در معنای محدودِ صرفاً \gls{data-ethics} تجاری (همچنین نگاه کنید به \lr{Taylor \& Dencik, 2020}). برای مثال، این چشم‌انداز وسیع مستلزم آن است که قطعاً نقشی برای اخلاق‌شناسان—و همچنین سایر دانشگاهیان، مانند فیلسوفان، دانشمندان علوم اجتماعی و محققان مطالعات علم و فناوری \lr{(STS)}—وجود دارد تا دانش خود را با مخاطبان وسیع‌تری به اشتراک بگذارند و بحث‌های عمومی و سیاسی را آگاه سازند. اخلاق می‌تواند برای مثال «به عنوان یک چشم‌انداز فرا-سطحی عمل کند تا هرگونه اختلاف نظر مربوط به حکمرانی فناوری را در نظر بگیرد» و «لایه‌ای از تفکر اصولی دقیق را به بحث‌های مملو از ارزش اضافه کند» \lr{(Bietti, 2020, p. 5)}.

«اخلاق از درون» نیز باید بخشی از این بحث عمومی باشد، زیرا ارزش زیادی در تجربه اول‌شخص و دانشِ کارمندان حوزه فناوری—مانند دانشمندان داده—که در واقع در حال ساخت فناوری‌های داده‌محور هستند، نهفته است. و نکته آخر اما نه کم‌اهمیت، جوامعی که ممکن است تحت تأثیر این رویه‌های علم داده قرار گیرند باید شنیده شوند و مورد مشورت قرار گیرند تا به فضای عملی برسیم که نه تنها منافع شرکت‌های داده‌محور را حفظ کند، بلکه در درجه اول و پیش از همه، منافع شهروندان را در قلب خود داشته باشد \lr{(Taylor \& Dencik, 2020)}. ناگفته پیداست که سازماندهی چنین فرآیندهای دموکراتیکی سخت و دشوار است. فراهم کردن یک زمین بازی هموار که در آن تمام این صداها واقعاً شنیده شوند، یک چالش اجتماعی و سیاسی است که نیازمند سرعتی آهسته‌تر از آن چیزی است که برخی از تکنولوژی-خوش‌بین‌ها (از جمله شرکت‌های داده‌محور) احتمالاً به آن امیدوارند.

در مجموع، از آنجا که \gls{data-science} (هم به عنوان یک حوزه پژوهشی و هم فعالیت تجاری) هم‌اکنون تأثیر بنیادینی بر سازماندهی جامعه دارد و خواهد داشت، اکنون زمان آن است که این قدرت را نقادانه ارزیابی کنیم و اطمینان حاصل کنیم که کنترل و توازن‌های کافی \lr{(checks and balances)} در جای خود قرار دارند تا تضمین شود که این علم واقعاً به یک جامعه شکوفا کمک خواهد کرد، نه اینکه آن را از میان ببرد. \gls{data-ethics} قطعاً می‌تواند به توسعه دانشمندان داده و کسب‌وکارهای داده‌محور مسئول‌تر و پاسخگوتر کمک کند، اما باید همواره در درون فضای عملِ فراهم‌شده توسط \gls{law} قرار گیرد.

\vfill

% =================================================================
% ۲۰.۵ اخلاق داده و علم داده: آیا این همراهی پایدار است؟
% =================================================================
\subsection*{۲۰.۵ \gls{data-ethics} و \gls{data-science}: آیا این همراهی پایدار است؟}
\addcontentsline{toc}{subsection}{۲۰.۵ \gls{data-ethics} و \gls{data-science}: آیا این همراهی پایدار است؟}
\label{sec:20-long-run}

این فصل «اخلاق داده و علم داده: یک ازدواج ناآرام؟» نام‌گذاری شده است. تا کنون، شما باید قادر باشید درک کنید که این «ازدواج ناآرام» به چه چیزی اشاره دارد. از یک سو، آشکارا روشن است که \gls{data-science} و کسب‌وکارهای داده‌محور تأثیر اخلاقی قابل‌توجهی بر جامعه ما دارند. آن‌ها جنبه‌های کلیدی زندگی روزمره را واسطه‌گری می‌کنند: از آموزش تا روابط اجتماعی، از تعامل ما با دولت تا شیوه‌ای که اخبارمان را دریافت می‌کنیم. در نتیجه، منطقی به نظر می‌رسد که ما \gls{data-science} و کسب‌وکارهایی را که پیش می‌برد، به گونه‌ای طراحی و سازماندهی کنیم که منعکس‌کننده ارزش‌های اخلاقی‌ای باشد که برایمان اهمیت دارد. \gls{data-ethics} می‌تواند در انجام این کار کمک کند. در نگاه اول، \gls{data-science} و \gls{data-ethics} یک زوج کامل هستند.

با این حال، در عین حال، ما همچنین تثبیت کردیم که \gls{data-ethics} یک چیز همگن (یکدست) نیست. این یک حوزه پژوهشی در اخلاق آکادمیک و یک استراتژی تجاری است، و گاهی این دو با هم می‌آیند. می‌تواند بر افراد، فناوری و سازمانِ یک کسب‌وکار متمرکز باشد. می‌تواند برای تفسیر فضای عملی که قانون فراهم می‌کند استفاده شود، و می‌تواند برای دور زدن مقررات قانونی مورد سوءاستفاده قرار گیرد.

آنچه \gls{data-ethics} را—به‌ویژه در حوزه تجاری—بسیار جذاب می‌کند، انعطاف‌پذیری آن است! شانسِ انجامِ «کار درست!» معلوم می‌شود که بزرگترین نقطه ضعف آن نیز هست. همان‌طور که مشخص است، \gls{data-ethics} زمانی که برای فراهم کردن قطعیت (فصل‌الخطاب) به کار گرفته می‌شود، به شدت دچار بار اضافی می‌گردد، زیرا ماهیت باز-پایان آن در واقع از چنین کاربردی پشتیبانی نمی‌کند.

شاید \gls{data-science}، که سر از پا نمی‌شناخت، بدون درک کامل از اینکه \gls{data-ethics} واقعاً درباره چیست، با شتاب وارد این رابطه شد. به هر حال، \gls{data-ethics} و همچنین \gls{data-science} هر دو هنوز نسبتاً جوان هستند. با این حال، در تاریک‌ترین سناریو، \gls{data-ethics} در دستان کسب‌وکارهای داده‌محور تبدیل به وسیله‌ای برای رفتار عمدی و مخرب می‌شود، زمانی که برای مانع‌تراشی و دور انداختن مقررات قانونی استفاده گردد.

روی‌هم‌رفته، کاملاً روشن می‌شود که \gls{data-ethics} و \gls{data-science} هنوز به جایگاهی آرام نرسیده‌اند. بنابراین، برای سال‌های پیش رو چه انتظاری می‌توانیم داشته باشیم؟ به عنوان راهی برای نتیجه‌گیری این فصل، ما به‌طور مختصر به سه تحولی که انتظار داریم این رابطه به خود بگیرد، خواهیم پرداخت.

\vfill

% =================================================================
% نتیجه‌گیری (داخل کادر خاکستری)
% =================================================================
\vspace{0.5cm}
\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\begin{center}
			\textbf{\Large نتیجه‌گیری: سه تحول مورد انتظار}
		\end{center}
		\vspace{0.3cm}
		
		\textbf{۱. مقررات جدید} \\
		برخی مسائل مهم‌تر از آن هستند که به صلاحدید شرکت‌ها واگذار شوند. تا زمانی که \gls{data-science} بر داده‌هایی استوار است که عمدتاً از جامعه تأمین می‌شوند و خروجی‌های آن بر جامعه تأثیر می‌گذارد—که هر دو از ویژگی‌های ذاتی این رشته و حرفه هستند—\gls{data-science} باید به‌طور مؤثری تنظیم (رگولاتوری) شود تا اطمینان حاصل گردد که برای جامعه سودمند است. این شامل اثرات اجتماعی، سیاسی و اقتصادی آن می‌شود: بحث درباره اینکه این سودمندی شامل چه چیزهایی است، به همان اندازه که وظیفه شرکت‌هاست، وظیفه جامعه نیز می‌باشد.
		
		مقررات، این ارتباط هنجاری بین کسب‌وکار و جامعه را شکل می‌دهد، اما اثربخشی مقررات موجود در علم داده و فناوری داده به‌طور کلی، در حال حاضر به دلیل عدم انسجام بین شیوه‌های عملی و بدنه‌های قانونی محدود شده است. یک رویکرد یکپارچه به مقررات، قوانین مصرف‌کننده و خصوصی و همچنین حفاظت از داده‌ها را در نظر می‌گیرد و به حقوق بشر و مقررات قابل اجرای بین‌المللی برای علم داده‌ای که در مرزها و کشورهای مختلف انجام می‌شود، متصل می‌گردد. پاسخگویی باید بعدی بین‌المللی پیدا کند، همان‌طور که شکست قانون، مقررات و سیاست در مقابله با تخلفات شرکت‌هایی مانند «کمبریج آنالیتیکا» \lr{(Cambridge Analytica)} نشان‌دهنده این نیاز است.
		
		\vspace{0.3cm}
		\hrule
		\vspace{0.3cm}
		
		\textbf{۲. دانشمندان داده مسئول و پاسخگو} \\
		می‌توان انتظار داشت که \gls{data-science} به عنوان یک حرفه بلوغ یابد و با گذشت زمان، الزامات حرفه‌ای برای آن ایجاد شود. این الزامات نه تنها به استانداردهای فنی کار، بلکه به تأثیر اجتماعی آن نیز اشاره خواهند داشت. این بدین معناست که دانشمندان داده باید راه‌هایی برای تعامل با عاملیت اجتماعی و همچنین سیاسی خود بیابند. این امر فراتر از «نیت‌های خوب» می‌رود و شامل «ارزیابی‌های دقیق» و نیاز به تصریح «تعهدات سیاسی» خواهد بود \lr{(Green, 2018, p. 45)}.
		
		این موضوع همچنین منجر به تمرکز بیشتر بر مسئولیت شخصی و همچنین پاسخگویی خواهد شد. در چنین سیستمی، زمانی که راه‌حل‌های داده‌محور نتوانند به استانداردهای خاصی پایبند باشند یا ثابت شود که استانداردهای حرفه‌ای خاصی در فرآیند طراحی رعایت نشده‌اند، متخصصان در سطح شخصی—و نه صرفاً در سطح شرکتی—پاسخگو خواهند بود. علاوه بر این، پیوند دادن پاسخگویی به مسئولیت ضروری است. رویکرد فعلی علوم کامپیوتر به «داده‌های مسئولانه» ماهیتی عمدتاً فنی دارد و بر الزامات محدود و رسمی تمرکز می‌کند تا تقاضاهای پیچیده‌ترِ پاسخگویی دموکراتیک و قانونی. به این ترتیب، این رویکرد در حال حاضر برای اطمینان از سودرسانی یا جبران خسارت در جایی که حقوق یا اصول نقض می‌شوند، ناکافی است.
		
		\vspace{0.3cm}
		\hrule
		\vspace{0.3cm}
		
		\textbf{۳. تقاضای اجتماعی: کسب‌وکارهای داده‌محور ۲.۰} \\
		انتظار می‌رود که در سال‌های آینده، آگاهی اجتماعی از نفوذ و مشارکت‌های \gls{data-science} بیشتر رشد کند. این می‌تواند منجر به افزایش شهروندان منتقدی شود که از نهادهای دولتی خود مقررات سخت‌گیرانه‌تر و اجرای قوی‌تر را مطالبه کنند. علاوه بر این، شهروندان در نقش خود به عنوان مصرف‌کننده، می‌توانند شرکت‌های داده‌محور را تحت فشار قرار دهند تا در رویه‌های تجاری مسئولانه سرمایه‌گذاری کنند. بیان نگرانی‌ها و مطالبه خدمات جدید ممکن است در واقع فشار بزرگی برای اصلاح کسب‌وکارها باشد.
		
		با این حال، اگر مشتریان و کارمندان اعتماد خود را از دست بدهند، ممکن است تصمیم بگیرند که «با پاهایشان رأی دهند» \lr{(Hirschmann, 1970)} (یعنی ترک کردن سرویس) و شرکت و خدماتی را که ارائه می‌دهد رها کنند و به دنبال جایگزین‌های منصفانه‌تر و دوستدار حریم خصوصی باشند. به خودی خود، این تقاضای رو به رشد برای رویه‌های \gls{data-science} اخلاقی‌تر احتمالاً برای اصلاح و/یا خاتمه دادن به مدل‌های کسب‌وکاره مشکوکی مانند رویکرد «پرداخت با داده‌های شما» یا رویه‌های غیرشفاف دلالان داده کافی نخواهد بود.
		
		با این حال، این امر می‌تواند توسعه مدل‌های کسب‌وکاره علم داده پایدارتر را تحریک کند و از مدل استارتاپی «سیلیکون‌ولی» که صرفاً بر «رشد سریع» متمرکز است، دور شود. شرکت‌های جدیدی که از شرِ ذینتِ «سریع حرکت کن و چیزها را بشکن» خلاص شوند، می‌توانند در واقع از همان ابتدا کسب‌وکاره خود را ساختاردهی کنند و محصولات و خدماتشان را با یک ذینتِ \gls{data-ethics} اصیل و روشن توسعه دهند. این کمک خواهد کرد تا اطمینان حاصل شود که \gls{data-ethics} به یک شریک برابر در این رابطه تبدیل می‌شود و صرفاً به عنوان یک وصله ناجورِ پرزرق‌وبرق پایان نمی‌یابد.
		\vspace{0.2cm}
	\end{minipage}
}

\vspace{2\baselineskip}

% =================================================================
% نکات بحث و گفتگو
% =================================================================
\noindent
\fcolorbox{black}{gray!10}{%
	\begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
		\vspace{0.2cm}
		\begin{center}
			\textbf{\Large نکات بحث و گفتگو}
		\end{center}
		\vspace{0.2cm}
		
		\begin{enumerate}
			\item \textbf{مسائل اخلاقی اصلی:} \\
			به نظر شما مسائل اخلاقی اصلی ناشی از \gls{data-science} چیست؟ در نظر بگیرید که چگونه علم داده قدرت و نفوذ خود را در رابطه با جامعه اعمال می‌کند، تأثیرات آن چگونه توزیع می‌شود، و جامعه مدنی چه نوع نفوذی می‌تواند بر کار دانشمندان داده اعمال کند.
			
			\item \textbf{فضایل دانشمند داده:} \\
			فکر می‌کنید مهم‌ترین فضایلی که یک دانشمند داده باید در خود پرورش دهد چیست؟ نظریه‌های اخلاقی مطرح شده در این فصل را مرور کنید و بررسی نمایید که چگونه انتخاب‌های دانشمندان داده می‌تواند منجر به سودرسانی (خیررسانی) یا پتانسیل آسیب شود.
			
			\item \textbf{اخلاق به عنوان مزیت رقابتی:} \\
			آیا \gls{data-ethics} می‌تواند یک مزیت رقابتی برای یک شرکت باشد؟ معایب یا مزایای بالقوه برای شرکت‌هایی را ارزیابی کنید که کار علم داده‌ای انجام می‌دهند که به عنوان محصول جانبی، به جامعه آسیب می‌رساند، و راه‌هایی که درگیر نشدن در چنین کارهایی ممکن است بر شرکت‌ها تأثیر بگذارد.
		\end{enumerate}
		\vspace{0.2cm}
	\end{minipage}
}

\vfill
\clearpage

% =================================================================
% پیام‌های کلیدی
% =================================================================
\vspace{0.5cm}

\section*{پیام‌های کلیدی}

\begin{itemize}
	\item \textbf{اخلاق} شاخه‌ای از فلسفه است که حول این پرسش می‌چرخد: «چگونه باید عمل کرد؟» به روشی سیستماتیک، این رشته دلایل و استانداردهای زیربنایی اعمال ما را مطالعه می‌کند و بررسی می‌کند که چه چیزی اعمال ما را از نظر اخلاقی درست یا غلط، و خوب یا بد می‌سازد.
	
	\item \textbf{\gls{data-ethics}} شاخه‌ای از اخلاق آکادمیک و همچنین یک استراتژی تجاری است که شرکت‌های داده‌محور توسعه می‌دهند تا با تأثیرات اجتماعی محصولات و خدمات خود مقابله کنند.
	
	\item آنچه یک \textbf{هنجار قانونی} را از یک هنجار اخلاقی متمایز می‌کند، این است که قانون «قابل پیش‌بینی» و «قابل اجرا» است. به عبارت دیگر، شما از پیش می‌دانید چه عملی مناسب تلقی می‌شود و اگر به قانون پایبند نباشید چه انتظاری باید داشته باشید. این نوعی از وضوح و قدرت است که اخلاق نمی‌تواند فراهم کند.
	
	\item زمانی که \gls{data-ethics} برای دور زدن توسعه یا اجرای مقررات استفاده شود، به آن \textbf{«اخلاق‌شویی»} \lr{(ethics washing)} گفته می‌شود.
\end{itemize}

\vfill
\clearpage

% =================================================================
% منابع
% =================================================================
\clearpage
\section*{منابع}
\addcontentsline{toc}{section}{منابع}
\label{sec:20-references}

% تنظیمات شکستن لینک‌ها
\def\UrlBreaks{\do\/\do-\do\.\do\_\do\=\do\&\do\?\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w\do\x\do\y\do\z\do\0\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9}

\begin{latin}
	\setlength{\parindent}{0pt}
	\setlength{\parskip}{0.3cm}
	\urlstyle{same} 
	
	Aristotle. (1984). \textit{The complete works of Aristotle: Revised Oxford edition}. Edited by Jonathan Barnes. Princeton University Press.
	
	Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., \& Rahwan, I. (2018). The Moral Machine experiment. \textit{Nature}, 563(7729):59–64.
	
	Bentham, J. ([1789] 1996). \textit{The collected works of Jeremy Bentham: An introduction to the principles of morals and legislation}. Clarendon Press.
	
	Bietti, E. (2020). From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy. In: \textit{Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency}.
	
	Binns, R. (2018). Fairness in machine learning: Lessons from political philosophy. In: \textit{Proceedings of the 2018 Conference on Fairness, Accountability and Transparency} (pp. 149–159).
	
	BVV. (2018). Wat is Syri. Retrieved from \url{https://bijvoorbaatverdacht.nl/wat-is-syri/}.
	
	Chen, B., \& Zhu, H. (2019). Towards value-sensitive learning analytics design. In: \textit{Proceedings of the 9th International Conference on Learning Analytics \& Knowledge}.
	
	Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. \textit{Big Data}, 5(2), 153–163.
	
	Floridi, L., \& Taddeo, M. (2016). What is data ethics? \textit{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, 374(2083), 20160360. \url{https://doi.org/10.1098/rsta.2016.0360}
	
	Green, Ben. (2018). Data science as political action: grounding data science in a politics of justice. arXiv preprint arXiv:1811.03435.
	
	Grgić-Hlača, N. (2018). Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian.
	
	Guardian. (2019). Palantir has no place at Berkeley: They help tear immigrant families apart. \textit{The Guardian}. Retrieved from \url{http://www.theguardian.com/commentisfree/2019/may/31/palantirberkeley-immigrant-families-apart}.
	
	Guardian. (2020). Hundreds of Amazon warehouse workers to call in sick in coronavirus protest. \textit{The Guardian}. Retrieved from \url{http://www.theguardian.com/technology/2020/apr/20/amazonwarehouse-workers-sickout-coronavirus}.
	
	Hagendorff, T. (2020). The ethics of AI ethics: An evaluation of guidelines. \textit{Mind. Machines}: 1-22.
	
	Hasselbalch, G., \& Tranberg, P. (2016). \textit{Data ethics: The new competitive advantage}. Publishare.
	
	Hicks, Mar. (2018, November 9). The long history behind the Google Walkout. \textit{The Verge}. Retrieved from \url{https://www.theverge.com/2018/11/9/18078664/google-walkout-history-tech-strikes-labororganizing}.
	
	Hildebrandt, M. (2020). \textit{Law for computer scientists and other folk}. Oxford University Press.
	
	Hirschmann, A. O. (1970). \textit{Exit, voice, and loyalty: Responses to decline in firms, organizations, and states}. Harvard University Press.
	
	Jobin, A., Ienca, M., \& Vayena, E. (2019). The global landscape of AI ethics guidelines. \textit{Nature Machine Intelligence}, 1(9), 389–399.
	
	Kant, I. ([1785] 1997). \textit{Groundwork of the metaphysics of morals}. Translated by Mary Gregor. Cambridge: Cambridge University Press.
	
	Keymolen, E. (2016). \textit{Trust on the line. A philosophical exploration of trust in the networked era}. Wolf Legal Publisher.
	
	Keymolen, E. (2017). Trust in the Networked Era: When Phones Become Hotel Keys. \textit{Techné: Research in Philosophy and Technology}, 22(1), 51–75.
	
	Leonelli, S. (2016). Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems. \textit{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, 374(2083), 20160122.
	
	Microsoft. (2020). Responsible AI principles. Retrieved from \url{https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6}
	
	Mittelstadt, B. (2019). AI Ethics—Too Principled to Fail? arXiv preprint arXiv:1906.06668.
	
	NBC News. (2020). Controversial tech company pitches facial recognition to track COVID-19. Retrieved from \url{https://www.nbcnews.com/now/video/controversial-tech-company-pitches-facialrecognition-to-track-covid-19-82638917537}.
	
	New Statesman. (2018). Cambridge Analytica and the digital war in Africa. Retrieved from \url{https://www.newstatesman.com/world/2018/03/cambridge-analytica-facebook-elections-africa-kenya}.
	
	New York Times. (2019). SpaceX Unveils Silvery Vision to Mars: ‘It’s Basically an I.C.B.M. That Lands.’ \textit{The New York Times}. Retrieved from \url{https://www.nytimes.com/2019/09/29/science/elonmusk-spacex-starship.html}.
	
	Nyholm, S. (2018). The ethics of crashes with self-driving cars: A roadmap, I. \textit{Philosophy Compass}, 13(7), e12507.
	
	Orr, W., \& Davis, J. L. (2020). Attributions of ethical responsibility by Artificial Intelligence practitioners. \textit{Information, Communication \& Society}: 1–17.
	
	Observer. (2020, January 4). Fresh Cambridge Analytica leak ‘shows global manipulation is out of control.’ \textit{The Observer}. Retrieved from \url{https://www.theguardian.com/uk-news/2020/jan/04/cambridge-analytica-data-leak-global-election-manipulation}.
	
	Oxford-Munich. (2020). Code of conduct. Retrieved from \url{http://www.code-of-ethics.org/code-ofconduct/}.
	
	PartnershipOnAI. (2019). Human-AI collaboration framework and case studies. Retrieved from \url{https://www.partnershiponai.org/wp-content/uploads/2019/09/CPAIS-Framework-and-CaseStudies-9-23.pdf}.
	
	RSS, and IFoA. (2019). A Guide for Ethical Data Science. Retrieved from \url{https://www.actuaries.org.uk/system/files/field/document/An\%20Ethical\%20Charter\%20for\%20Date\%20Science\%20WEB\%20FINAL.PDF}.
	
	Saltz, J. S., \& Dewar, N. (2019). Data science ethical considerations: a systematic literature review and proposed project framework. \textit{Ethics and Information Technology}, 21(3), 197–208.
	
	Schiff, D., Biddle, J., Borenstein, J., \& Laas, K. (2020). What’s next for AI ethics, policy, and governance? A global overview. In: \textit{Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}.
	
	Simon, J. (2017). Value-sensitive design and responsible research and innovation. In S. O. Hansson (Ed.), \textit{The ethics of technology: Methods and approaches} (pp. 219–236). Rowman \& Littlefield.
	
	Sinnott-Armstrong, W. (2019). Consequentialism. In E. N. Zalta (Ed.), \textit{The Stanford Encyclopedia of Philosophy} (Summer 2019). Stanford University. Retrieved from \url{https://plato.stanford.edu/archives/sum2019/entries/consequentialism/}.
	
	Smits, M., Bredie, B., van Goor, H., \& Verbeek, P.-P. (2019). Values that matter: Mediation theory and Design for Values. In: \textit{Academy for Design Innovation Management Conference 2019: Research perspectives in the era of Transformations}.
	
	Suresh, H., \& Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002.
	
	Tamanaha, B. Z. (2007). \textit{A concise guide to the rule of law}. FLORENCE WORKSHOP ON THE RULE OF LAW.
	
	Tamanaha, B. Z. (2004). \textit{On the rule of law: History, politics}. Cambridge University Press.
	
	Taylor, L., \& Dencik, L. (2020). Constructing commercial data ethics. \textit{Technology and Regulation}: 1–10.
	
	Timmons, M. (2012). \textit{Moral theory: An introduction}. Rowman \& Littlefield Publishers.
	
	Vallor, S. (2016). \textit{Technology and the Virtues. A philosophical guide to a future worth wanting}. Oxford University Press.
	
	Van de Poel, I. B. O., \& Royakkers, L. (2011). \textit{Ethics, technology, and engineering: An introduction}. Wiley.
	
	Wagner, B. (2018). Ethics as an escape from regulation: From ethics-washing to ethics-shopping. Being profiling. \textit{Cogitas ergo sum}: 84–90.
	
	Wired. (2018). When It Comes to Gorillas, Google Photos Remains Blind. \textit{Wired}. Retrieved from \url{https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/}.
	
	Wired. (2020). Scraping the Web Is a Powerful Tool. Clearview AI Abused It. \textit{Wired}. Retrieved from \url{https://www.wired.com/story/clearview-ai-scraping-web/}.
	
	Woollard, F., \& Howard-Snyder, F. (2002). Doing vs. Allowing Harm. In: Zalta, E.N., \& Nodelman, U. (eds). \textit{The Stanford Encyclopedia of Philosophy} (Winter 2022 Edition).
	
\end{latin}


